{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "dropout=0.75\n",
    "n_input_width=28\n",
    "n_input_height=28\n",
    "\n",
    "n_output  = 10 # e.g. MNIST total classes (0-9 digits)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input_width * n_input_height])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, n_input_width, n_input_width, 1])\n",
    "    \n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1)\n",
    "  \n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1':tf.Variable(tf.random_normal([5,5,1,32], stddev=0.1)),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2':tf.Variable(tf.random_normal([5,5,32, 64], stddev=0.1)),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024], stddev=0.1)),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32], stddev=0.1)),\n",
    "    'bc2':tf.Variable(tf.random_normal([64], stddev=0.1)),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024], stddev=0.1)),\n",
    "    'out': tf.Variable(tf.random_normal([10], stddev=0.1))\n",
    "\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 , batch Loss= 9.344152, Training Accuracy= 0.26000\n",
      "Epoch   2 , batch Loss= 9.660097, Training Accuracy= 0.31000\n",
      "Epoch   3 , batch Loss= 9.723860, Training Accuracy= 0.28000\n",
      "Epoch   4 , batch Loss= 6.537767, Training Accuracy= 0.35000\n",
      "Epoch   5 , batch Loss= 4.786735, Training Accuracy= 0.29000\n",
      "Epoch   6 , batch Loss= 2.994353, Training Accuracy= 0.39000\n",
      "Epoch   7 , batch Loss= 1.884765, Training Accuracy= 0.56000\n",
      "Epoch   8 , batch Loss= 2.016315, Training Accuracy= 0.51000\n",
      "Epoch   9 , batch Loss= 1.464054, Training Accuracy= 0.60000\n",
      "Epoch   10 , batch Loss= 1.221941, Training Accuracy= 0.66000\n",
      "Epoch   11 , batch Loss= 1.056713, Training Accuracy= 0.75000\n",
      "Epoch   12 , batch Loss= 1.079681, Training Accuracy= 0.71000\n",
      "Epoch   13 , batch Loss= 1.369792, Training Accuracy= 0.58000\n",
      "Epoch   14 , batch Loss= 1.013355, Training Accuracy= 0.66000\n",
      "Epoch   15 , batch Loss= 0.702776, Training Accuracy= 0.80000\n",
      "Epoch   16 , batch Loss= 0.381085, Training Accuracy= 0.89000\n",
      "Epoch   17 , batch Loss= 0.589686, Training Accuracy= 0.87000\n",
      "Epoch   18 , batch Loss= 0.519039, Training Accuracy= 0.83000\n",
      "Epoch   19 , batch Loss= 0.791116, Training Accuracy= 0.78000\n",
      "Epoch   20 , batch Loss= 0.556843, Training Accuracy= 0.79000\n",
      "Epoch   21 , batch Loss= 0.611913, Training Accuracy= 0.79000\n",
      "Epoch   22 , batch Loss= 0.560455, Training Accuracy= 0.85000\n",
      "Epoch   23 , batch Loss= 0.784313, Training Accuracy= 0.77000\n",
      "Epoch   24 , batch Loss= 0.448315, Training Accuracy= 0.91000\n",
      "Epoch   25 , batch Loss= 0.527842, Training Accuracy= 0.89000\n",
      "Epoch   26 , batch Loss= 0.574122, Training Accuracy= 0.86000\n",
      "Epoch   27 , batch Loss= 0.579353, Training Accuracy= 0.81000\n",
      "Epoch   28 , batch Loss= 0.539599, Training Accuracy= 0.87000\n",
      "Epoch   29 , batch Loss= 0.423196, Training Accuracy= 0.88000\n",
      "Epoch   30 , batch Loss= 0.413138, Training Accuracy= 0.84000\n",
      "Epoch   31 , batch Loss= 0.309954, Training Accuracy= 0.90000\n",
      "Epoch   32 , batch Loss= 0.355024, Training Accuracy= 0.88000\n",
      "Epoch   33 , batch Loss= 0.459110, Training Accuracy= 0.88000\n",
      "Epoch   34 , batch Loss= 0.314185, Training Accuracy= 0.91000\n",
      "Epoch   35 , batch Loss= 0.434177, Training Accuracy= 0.88000\n",
      "Epoch   36 , batch Loss= 0.227551, Training Accuracy= 0.93000\n",
      "Epoch   37 , batch Loss= 0.353039, Training Accuracy= 0.87000\n",
      "Epoch   38 , batch Loss= 0.602122, Training Accuracy= 0.82000\n",
      "Epoch   39 , batch Loss= 0.449621, Training Accuracy= 0.86000\n",
      "Epoch   40 , batch Loss= 0.356993, Training Accuracy= 0.87000\n",
      "Epoch   41 , batch Loss= 0.206833, Training Accuracy= 0.95000\n",
      "Epoch   42 , batch Loss= 0.295178, Training Accuracy= 0.90000\n",
      "Epoch   43 , batch Loss= 0.344834, Training Accuracy= 0.91000\n",
      "Epoch   44 , batch Loss= 0.322734, Training Accuracy= 0.90000\n",
      "Epoch   45 , batch Loss= 0.338806, Training Accuracy= 0.93000\n",
      "Epoch   46 , batch Loss= 0.260214, Training Accuracy= 0.95000\n",
      "Epoch   47 , batch Loss= 0.275558, Training Accuracy= 0.91000\n",
      "Epoch   48 , batch Loss= 0.279119, Training Accuracy= 0.92000\n",
      "Epoch   49 , batch Loss= 0.178360, Training Accuracy= 0.94000\n",
      "Epoch   50 , batch Loss= 0.111861, Training Accuracy= 0.96000\n",
      "Epoch   51 , batch Loss= 0.269092, Training Accuracy= 0.93000\n",
      "Epoch   52 , batch Loss= 0.147406, Training Accuracy= 0.96000\n",
      "Epoch   53 , batch Loss= 0.438236, Training Accuracy= 0.83000\n",
      "Epoch   54 , batch Loss= 0.168058, Training Accuracy= 0.94000\n",
      "Epoch   55 , batch Loss= 0.110512, Training Accuracy= 0.97000\n",
      "Epoch   56 , batch Loss= 0.072090, Training Accuracy= 0.99000\n",
      "Epoch   57 , batch Loss= 0.109813, Training Accuracy= 0.97000\n",
      "Epoch   58 , batch Loss= 0.266747, Training Accuracy= 0.90000\n",
      "Epoch   59 , batch Loss= 0.211047, Training Accuracy= 0.94000\n",
      "Epoch   60 , batch Loss= 0.210746, Training Accuracy= 0.95000\n",
      "Epoch   61 , batch Loss= 0.146292, Training Accuracy= 0.94000\n",
      "Epoch   62 , batch Loss= 0.141769, Training Accuracy= 0.96000\n",
      "Epoch   63 , batch Loss= 0.209879, Training Accuracy= 0.95000\n",
      "Epoch   64 , batch Loss= 0.126005, Training Accuracy= 0.95000\n",
      "Epoch   65 , batch Loss= 0.114981, Training Accuracy= 0.96000\n",
      "Epoch   66 , batch Loss= 0.263564, Training Accuracy= 0.89000\n",
      "Epoch   67 , batch Loss= 0.216146, Training Accuracy= 0.93000\n",
      "Epoch   68 , batch Loss= 0.344542, Training Accuracy= 0.90000\n",
      "Epoch   69 , batch Loss= 0.167121, Training Accuracy= 0.94000\n",
      "Epoch   70 , batch Loss= 0.125191, Training Accuracy= 0.95000\n",
      "Epoch   71 , batch Loss= 0.135864, Training Accuracy= 0.96000\n",
      "Epoch   72 , batch Loss= 0.074165, Training Accuracy= 0.99000\n",
      "Epoch   73 , batch Loss= 0.173011, Training Accuracy= 0.94000\n",
      "Epoch   74 , batch Loss= 0.220893, Training Accuracy= 0.93000\n",
      "Epoch   75 , batch Loss= 0.115759, Training Accuracy= 0.95000\n",
      "Epoch   76 , batch Loss= 0.332098, Training Accuracy= 0.91000\n",
      "Epoch   77 , batch Loss= 0.377233, Training Accuracy= 0.89000\n",
      "Epoch   78 , batch Loss= 0.175589, Training Accuracy= 0.94000\n",
      "Epoch   79 , batch Loss= 0.182936, Training Accuracy= 0.93000\n",
      "Epoch   80 , batch Loss= 0.185114, Training Accuracy= 0.92000\n",
      "Epoch   81 , batch Loss= 0.287637, Training Accuracy= 0.92000\n",
      "Epoch   82 , batch Loss= 0.290121, Training Accuracy= 0.94000\n",
      "Epoch   83 , batch Loss= 0.096818, Training Accuracy= 0.97000\n",
      "Epoch   84 , batch Loss= 0.171949, Training Accuracy= 0.94000\n",
      "Epoch   85 , batch Loss= 0.140081, Training Accuracy= 0.94000\n",
      "Epoch   86 , batch Loss= 0.139224, Training Accuracy= 0.97000\n",
      "Epoch   87 , batch Loss= 0.154891, Training Accuracy= 0.97000\n",
      "Epoch   88 , batch Loss= 0.169087, Training Accuracy= 0.94000\n",
      "Epoch   89 , batch Loss= 0.126738, Training Accuracy= 0.97000\n",
      "Epoch   90 , batch Loss= 0.202148, Training Accuracy= 0.94000\n",
      "Epoch   91 , batch Loss= 0.103941, Training Accuracy= 0.97000\n",
      "Epoch   92 , batch Loss= 0.153068, Training Accuracy= 0.97000\n",
      "Epoch   93 , batch Loss= 0.182836, Training Accuracy= 0.96000\n",
      "Epoch   94 , batch Loss= 0.283411, Training Accuracy= 0.90000\n",
      "Epoch   95 , batch Loss= 0.099848, Training Accuracy= 0.97000\n",
      "Epoch   96 , batch Loss= 0.176343, Training Accuracy= 0.95000\n",
      "Epoch   97 , batch Loss= 0.099347, Training Accuracy= 0.98000\n",
      "Epoch   98 , batch Loss= 0.298286, Training Accuracy= 0.92000\n",
      "Epoch   99 , batch Loss= 0.111592, Training Accuracy= 0.96000\n",
      "Epoch   100 , batch Loss= 0.082335, Training Accuracy= 0.97000\n",
      "Epoch   101 , batch Loss= 0.074535, Training Accuracy= 0.99000\n",
      "Epoch   102 , batch Loss= 0.197909, Training Accuracy= 0.92000\n",
      "Epoch   103 , batch Loss= 0.075885, Training Accuracy= 0.98000\n",
      "Epoch   104 , batch Loss= 0.087818, Training Accuracy= 0.97000\n",
      "Epoch   105 , batch Loss= 0.136332, Training Accuracy= 0.96000\n",
      "Epoch   106 , batch Loss= 0.064537, Training Accuracy= 0.98000\n",
      "Epoch   107 , batch Loss= 0.066591, Training Accuracy= 0.99000\n",
      "Epoch   108 , batch Loss= 0.224296, Training Accuracy= 0.91000\n",
      "Epoch   109 , batch Loss= 0.157187, Training Accuracy= 0.96000\n",
      "Epoch   110 , batch Loss= 0.184036, Training Accuracy= 0.95000\n",
      "Epoch   111 , batch Loss= 0.102052, Training Accuracy= 0.96000\n",
      "Epoch   112 , batch Loss= 0.081945, Training Accuracy= 0.98000\n",
      "Epoch   113 , batch Loss= 0.046940, Training Accuracy= 0.99000\n",
      "Epoch   114 , batch Loss= 0.076807, Training Accuracy= 0.99000\n",
      "Epoch   115 , batch Loss= 0.122335, Training Accuracy= 0.96000\n",
      "Epoch   116 , batch Loss= 0.132892, Training Accuracy= 0.96000\n",
      "Epoch   117 , batch Loss= 0.120346, Training Accuracy= 0.97000\n",
      "Epoch   118 , batch Loss= 0.148253, Training Accuracy= 0.94000\n",
      "Epoch   119 , batch Loss= 0.077602, Training Accuracy= 0.98000\n",
      "Epoch   120 , batch Loss= 0.132676, Training Accuracy= 0.95000\n",
      "Epoch   121 , batch Loss= 0.110577, Training Accuracy= 0.94000\n",
      "Epoch   122 , batch Loss= 0.140575, Training Accuracy= 0.95000\n",
      "Epoch   123 , batch Loss= 0.126791, Training Accuracy= 0.96000\n",
      "Epoch   124 , batch Loss= 0.018856, Training Accuracy= 1.00000\n",
      "Epoch   125 , batch Loss= 0.136874, Training Accuracy= 0.97000\n",
      "Epoch   126 , batch Loss= 0.149229, Training Accuracy= 0.96000\n",
      "Epoch   127 , batch Loss= 0.064917, Training Accuracy= 0.99000\n",
      "Epoch   128 , batch Loss= 0.165946, Training Accuracy= 0.95000\n",
      "Epoch   129 , batch Loss= 0.101855, Training Accuracy= 0.97000\n",
      "Epoch   130 , batch Loss= 0.126466, Training Accuracy= 0.97000\n",
      "Epoch   131 , batch Loss= 0.128274, Training Accuracy= 0.96000\n",
      "Epoch   132 , batch Loss= 0.069507, Training Accuracy= 0.98000\n",
      "Epoch   133 , batch Loss= 0.039652, Training Accuracy= 1.00000\n",
      "Epoch   134 , batch Loss= 0.140009, Training Accuracy= 0.97000\n",
      "Epoch   135 , batch Loss= 0.094209, Training Accuracy= 0.97000\n",
      "Epoch   136 , batch Loss= 0.072664, Training Accuracy= 0.99000\n",
      "Epoch   137 , batch Loss= 0.048115, Training Accuracy= 0.99000\n",
      "Epoch   138 , batch Loss= 0.070772, Training Accuracy= 0.98000\n",
      "Epoch   139 , batch Loss= 0.039936, Training Accuracy= 0.98000\n",
      "Epoch   140 , batch Loss= 0.052131, Training Accuracy= 0.99000\n",
      "Epoch   141 , batch Loss= 0.062543, Training Accuracy= 0.98000\n",
      "Epoch   142 , batch Loss= 0.110373, Training Accuracy= 0.97000\n",
      "Epoch   143 , batch Loss= 0.201734, Training Accuracy= 0.96000\n",
      "Epoch   144 , batch Loss= 0.121849, Training Accuracy= 0.96000\n",
      "Epoch   145 , batch Loss= 0.046690, Training Accuracy= 0.99000\n",
      "Epoch   146 , batch Loss= 0.126986, Training Accuracy= 0.96000\n",
      "Epoch   147 , batch Loss= 0.031426, Training Accuracy= 1.00000\n",
      "Epoch   148 , batch Loss= 0.093962, Training Accuracy= 0.98000\n",
      "Epoch   149 , batch Loss= 0.111410, Training Accuracy= 0.95000\n",
      "Epoch   150 , batch Loss= 0.042358, Training Accuracy= 0.99000\n",
      "Epoch   151 , batch Loss= 0.133935, Training Accuracy= 0.93000\n",
      "Epoch   152 , batch Loss= 0.132169, Training Accuracy= 0.94000\n",
      "Epoch   153 , batch Loss= 0.086329, Training Accuracy= 0.96000\n",
      "Epoch   154 , batch Loss= 0.060402, Training Accuracy= 0.99000\n",
      "Epoch   155 , batch Loss= 0.043679, Training Accuracy= 0.98000\n",
      "Epoch   156 , batch Loss= 0.075695, Training Accuracy= 0.99000\n",
      "Epoch   157 , batch Loss= 0.143410, Training Accuracy= 0.98000\n",
      "Epoch   158 , batch Loss= 0.180415, Training Accuracy= 0.95000\n",
      "Epoch   159 , batch Loss= 0.054426, Training Accuracy= 0.99000\n",
      "Epoch   160 , batch Loss= 0.136419, Training Accuracy= 0.96000\n",
      "Epoch   161 , batch Loss= 0.077460, Training Accuracy= 0.97000\n",
      "Epoch   162 , batch Loss= 0.102953, Training Accuracy= 0.96000\n",
      "Epoch   163 , batch Loss= 0.076857, Training Accuracy= 0.98000\n",
      "Epoch   164 , batch Loss= 0.106555, Training Accuracy= 0.97000\n",
      "Epoch   165 , batch Loss= 0.046861, Training Accuracy= 1.00000\n",
      "Epoch   166 , batch Loss= 0.059745, Training Accuracy= 0.98000\n",
      "Epoch   167 , batch Loss= 0.082973, Training Accuracy= 0.98000\n",
      "Epoch   168 , batch Loss= 0.036931, Training Accuracy= 0.99000\n",
      "Epoch   169 , batch Loss= 0.070311, Training Accuracy= 0.97000\n",
      "Epoch   170 , batch Loss= 0.095268, Training Accuracy= 0.97000\n",
      "Epoch   171 , batch Loss= 0.058353, Training Accuracy= 0.98000\n",
      "Epoch   172 , batch Loss= 0.112977, Training Accuracy= 0.95000\n",
      "Epoch   173 , batch Loss= 0.071773, Training Accuracy= 0.97000\n",
      "Epoch   174 , batch Loss= 0.038403, Training Accuracy= 0.99000\n",
      "Epoch   175 , batch Loss= 0.088687, Training Accuracy= 0.96000\n",
      "Epoch   176 , batch Loss= 0.160581, Training Accuracy= 0.95000\n",
      "Epoch   177 , batch Loss= 0.108272, Training Accuracy= 0.94000\n",
      "Epoch   178 , batch Loss= 0.056001, Training Accuracy= 0.99000\n",
      "Epoch   179 , batch Loss= 0.049007, Training Accuracy= 0.98000\n",
      "Epoch   180 , batch Loss= 0.025143, Training Accuracy= 1.00000\n",
      "Epoch   181 , batch Loss= 0.166599, Training Accuracy= 0.95000\n",
      "Epoch   182 , batch Loss= 0.088461, Training Accuracy= 0.98000\n",
      "Epoch   183 , batch Loss= 0.033242, Training Accuracy= 0.99000\n",
      "Epoch   184 , batch Loss= 0.112308, Training Accuracy= 0.97000\n",
      "Epoch   185 , batch Loss= 0.059532, Training Accuracy= 0.97000\n",
      "Epoch   186 , batch Loss= 0.095703, Training Accuracy= 0.97000\n",
      "Epoch   187 , batch Loss= 0.104636, Training Accuracy= 0.97000\n",
      "Epoch   188 , batch Loss= 0.117304, Training Accuracy= 0.96000\n",
      "Epoch   189 , batch Loss= 0.090293, Training Accuracy= 0.99000\n",
      "Epoch   190 , batch Loss= 0.132930, Training Accuracy= 0.97000\n",
      "Epoch   191 , batch Loss= 0.065613, Training Accuracy= 0.98000\n",
      "Epoch   192 , batch Loss= 0.030988, Training Accuracy= 1.00000\n",
      "Epoch   193 , batch Loss= 0.054362, Training Accuracy= 0.98000\n",
      "Epoch   194 , batch Loss= 0.017456, Training Accuracy= 0.99000\n",
      "Epoch   195 , batch Loss= 0.076834, Training Accuracy= 0.97000\n",
      "Epoch   196 , batch Loss= 0.097422, Training Accuracy= 0.96000\n",
      "Epoch   197 , batch Loss= 0.078019, Training Accuracy= 0.96000\n",
      "Epoch   198 , batch Loss= 0.226224, Training Accuracy= 0.96000\n",
      "Epoch   199 , batch Loss= 0.050363, Training Accuracy= 0.99000\n",
      "Epoch   200 , batch Loss= 0.078744, Training Accuracy= 0.97000\n",
      "Epoch   201 , batch Loss= 0.056031, Training Accuracy= 0.99000\n",
      "Epoch   202 , batch Loss= 0.079545, Training Accuracy= 0.98000\n",
      "Epoch   203 , batch Loss= 0.078648, Training Accuracy= 0.97000\n",
      "Epoch   204 , batch Loss= 0.054342, Training Accuracy= 0.97000\n",
      "Epoch   205 , batch Loss= 0.061106, Training Accuracy= 0.97000\n",
      "Epoch   206 , batch Loss= 0.125306, Training Accuracy= 0.98000\n",
      "Epoch   207 , batch Loss= 0.085991, Training Accuracy= 0.99000\n",
      "Epoch   208 , batch Loss= 0.094556, Training Accuracy= 0.96000\n",
      "Epoch   209 , batch Loss= 0.131396, Training Accuracy= 0.96000\n",
      "Epoch   210 , batch Loss= 0.062640, Training Accuracy= 0.98000\n",
      "Epoch   211 , batch Loss= 0.061993, Training Accuracy= 0.97000\n",
      "Epoch   212 , batch Loss= 0.033213, Training Accuracy= 0.99000\n",
      "Epoch   213 , batch Loss= 0.022956, Training Accuracy= 0.99000\n",
      "Epoch   214 , batch Loss= 0.103261, Training Accuracy= 0.97000\n",
      "Epoch   215 , batch Loss= 0.114453, Training Accuracy= 0.95000\n",
      "Epoch   216 , batch Loss= 0.175918, Training Accuracy= 0.96000\n",
      "Epoch   217 , batch Loss= 0.185461, Training Accuracy= 0.96000\n",
      "Epoch   218 , batch Loss= 0.137896, Training Accuracy= 0.96000\n",
      "Epoch   219 , batch Loss= 0.184462, Training Accuracy= 0.96000\n",
      "Epoch   220 , batch Loss= 0.027238, Training Accuracy= 0.99000\n",
      "Epoch   221 , batch Loss= 0.035224, Training Accuracy= 0.99000\n",
      "Epoch   222 , batch Loss= 0.154209, Training Accuracy= 0.98000\n",
      "Epoch   223 , batch Loss= 0.079244, Training Accuracy= 0.97000\n",
      "Epoch   224 , batch Loss= 0.016646, Training Accuracy= 1.00000\n",
      "Epoch   225 , batch Loss= 0.043850, Training Accuracy= 0.99000\n",
      "Epoch   226 , batch Loss= 0.126880, Training Accuracy= 0.97000\n",
      "Epoch   227 , batch Loss= 0.069045, Training Accuracy= 0.97000\n",
      "Epoch   228 , batch Loss= 0.074817, Training Accuracy= 0.99000\n",
      "Epoch   229 , batch Loss= 0.072506, Training Accuracy= 0.98000\n",
      "Epoch   230 , batch Loss= 0.036094, Training Accuracy= 1.00000\n",
      "Epoch   231 , batch Loss= 0.038367, Training Accuracy= 0.99000\n",
      "Epoch   232 , batch Loss= 0.144275, Training Accuracy= 0.94000\n",
      "Epoch   233 , batch Loss= 0.029825, Training Accuracy= 0.99000\n",
      "Epoch   234 , batch Loss= 0.167346, Training Accuracy= 0.95000\n",
      "Epoch   235 , batch Loss= 0.037907, Training Accuracy= 0.99000\n",
      "Epoch   236 , batch Loss= 0.062967, Training Accuracy= 0.99000\n",
      "Epoch   237 , batch Loss= 0.208412, Training Accuracy= 0.97000\n",
      "Epoch   238 , batch Loss= 0.150988, Training Accuracy= 0.96000\n",
      "Epoch   239 , batch Loss= 0.036162, Training Accuracy= 0.99000\n",
      "Epoch   240 , batch Loss= 0.049879, Training Accuracy= 0.99000\n",
      "Epoch   241 , batch Loss= 0.049020, Training Accuracy= 0.99000\n",
      "Epoch   242 , batch Loss= 0.125984, Training Accuracy= 0.97000\n",
      "Epoch   243 , batch Loss= 0.051837, Training Accuracy= 0.98000\n",
      "Epoch   244 , batch Loss= 0.093501, Training Accuracy= 0.97000\n",
      "Epoch   245 , batch Loss= 0.090942, Training Accuracy= 0.97000\n",
      "Epoch   246 , batch Loss= 0.039334, Training Accuracy= 0.98000\n",
      "Epoch   247 , batch Loss= 0.047483, Training Accuracy= 0.99000\n",
      "Epoch   248 , batch Loss= 0.054773, Training Accuracy= 1.00000\n",
      "Epoch   249 , batch Loss= 0.094324, Training Accuracy= 0.98000\n",
      "Epoch   250 , batch Loss= 0.097205, Training Accuracy= 0.97000\n",
      "Epoch   251 , batch Loss= 0.137800, Training Accuracy= 0.98000\n",
      "Epoch   252 , batch Loss= 0.078353, Training Accuracy= 0.98000\n",
      "Epoch   253 , batch Loss= 0.019558, Training Accuracy= 1.00000\n",
      "Epoch   254 , batch Loss= 0.020387, Training Accuracy= 0.99000\n",
      "Epoch   255 , batch Loss= 0.041709, Training Accuracy= 0.99000\n",
      "Epoch   256 , batch Loss= 0.084468, Training Accuracy= 0.95000\n",
      "Epoch   257 , batch Loss= 0.123040, Training Accuracy= 0.97000\n",
      "Epoch   258 , batch Loss= 0.040614, Training Accuracy= 0.99000\n",
      "Epoch   259 , batch Loss= 0.050179, Training Accuracy= 0.98000\n",
      "Epoch   260 , batch Loss= 0.046110, Training Accuracy= 0.99000\n",
      "Epoch   261 , batch Loss= 0.016555, Training Accuracy= 1.00000\n",
      "Epoch   262 , batch Loss= 0.086219, Training Accuracy= 0.95000\n",
      "Epoch   263 , batch Loss= 0.049219, Training Accuracy= 0.98000\n",
      "Epoch   264 , batch Loss= 0.118031, Training Accuracy= 0.96000\n",
      "Epoch   265 , batch Loss= 0.082815, Training Accuracy= 0.97000\n",
      "Epoch   266 , batch Loss= 0.127827, Training Accuracy= 0.96000\n",
      "Epoch   267 , batch Loss= 0.083087, Training Accuracy= 0.97000\n",
      "Epoch   268 , batch Loss= 0.169616, Training Accuracy= 0.95000\n",
      "Epoch   269 , batch Loss= 0.044638, Training Accuracy= 0.98000\n",
      "Epoch   270 , batch Loss= 0.105037, Training Accuracy= 0.99000\n",
      "Epoch   271 , batch Loss= 0.090570, Training Accuracy= 0.98000\n",
      "Epoch   272 , batch Loss= 0.032995, Training Accuracy= 0.99000\n",
      "Epoch   273 , batch Loss= 0.028860, Training Accuracy= 0.99000\n",
      "Epoch   274 , batch Loss= 0.118174, Training Accuracy= 0.96000\n",
      "Epoch   275 , batch Loss= 0.097894, Training Accuracy= 0.97000\n",
      "Epoch   276 , batch Loss= 0.053597, Training Accuracy= 0.98000\n",
      "Epoch   277 , batch Loss= 0.027689, Training Accuracy= 1.00000\n",
      "Epoch   278 , batch Loss= 0.059143, Training Accuracy= 0.99000\n",
      "Epoch   279 , batch Loss= 0.095449, Training Accuracy= 0.99000\n",
      "Epoch   280 , batch Loss= 0.010522, Training Accuracy= 1.00000\n",
      "Epoch   281 , batch Loss= 0.042038, Training Accuracy= 0.98000\n",
      "Epoch   282 , batch Loss= 0.046052, Training Accuracy= 0.98000\n",
      "Epoch   283 , batch Loss= 0.058685, Training Accuracy= 0.98000\n",
      "Epoch   284 , batch Loss= 0.094074, Training Accuracy= 0.98000\n",
      "Epoch   285 , batch Loss= 0.054115, Training Accuracy= 0.98000\n",
      "Epoch   286 , batch Loss= 0.045567, Training Accuracy= 0.99000\n",
      "Epoch   287 , batch Loss= 0.029430, Training Accuracy= 0.99000\n",
      "Epoch   288 , batch Loss= 0.108388, Training Accuracy= 0.98000\n",
      "Epoch   289 , batch Loss= 0.020270, Training Accuracy= 0.99000\n",
      "Epoch   290 , batch Loss= 0.049780, Training Accuracy= 0.98000\n",
      "Epoch   291 , batch Loss= 0.093356, Training Accuracy= 0.96000\n",
      "Epoch   292 , batch Loss= 0.021065, Training Accuracy= 0.99000\n",
      "Epoch   293 , batch Loss= 0.027198, Training Accuracy= 0.99000\n",
      "Epoch   294 , batch Loss= 0.047585, Training Accuracy= 0.99000\n",
      "Epoch   295 , batch Loss= 0.114963, Training Accuracy= 0.99000\n",
      "Epoch   296 , batch Loss= 0.116023, Training Accuracy= 0.97000\n",
      "Epoch   297 , batch Loss= 0.148351, Training Accuracy= 0.96000\n",
      "Epoch   298 , batch Loss= 0.131766, Training Accuracy= 0.96000\n",
      "Epoch   299 , batch Loss= 0.108425, Training Accuracy= 0.95000\n",
      "Epoch   300 , batch Loss= 0.075268, Training Accuracy= 0.98000\n",
      "Epoch   301 , batch Loss= 0.052898, Training Accuracy= 0.98000\n",
      "Epoch   302 , batch Loss= 0.071749, Training Accuracy= 0.99000\n",
      "Epoch   303 , batch Loss= 0.089774, Training Accuracy= 0.99000\n",
      "Epoch   304 , batch Loss= 0.049948, Training Accuracy= 0.99000\n",
      "Epoch   305 , batch Loss= 0.149911, Training Accuracy= 0.97000\n",
      "Epoch   306 , batch Loss= 0.019536, Training Accuracy= 1.00000\n",
      "Epoch   307 , batch Loss= 0.114699, Training Accuracy= 0.96000\n",
      "Epoch   308 , batch Loss= 0.030992, Training Accuracy= 0.99000\n",
      "Epoch   309 , batch Loss= 0.079250, Training Accuracy= 0.97000\n",
      "Epoch   310 , batch Loss= 0.047392, Training Accuracy= 0.98000\n",
      "Epoch   311 , batch Loss= 0.063496, Training Accuracy= 0.98000\n",
      "Epoch   312 , batch Loss= 0.124928, Training Accuracy= 0.96000\n",
      "Epoch   313 , batch Loss= 0.042559, Training Accuracy= 0.98000\n",
      "Epoch   314 , batch Loss= 0.022237, Training Accuracy= 1.00000\n",
      "Epoch   315 , batch Loss= 0.092742, Training Accuracy= 0.97000\n",
      "Epoch   316 , batch Loss= 0.020835, Training Accuracy= 1.00000\n",
      "Epoch   317 , batch Loss= 0.017620, Training Accuracy= 1.00000\n",
      "Epoch   318 , batch Loss= 0.127385, Training Accuracy= 0.96000\n",
      "Epoch   319 , batch Loss= 0.068049, Training Accuracy= 0.98000\n",
      "Epoch   320 , batch Loss= 0.026368, Training Accuracy= 0.99000\n",
      "Epoch   321 , batch Loss= 0.121075, Training Accuracy= 0.98000\n",
      "Epoch   322 , batch Loss= 0.052065, Training Accuracy= 0.98000\n",
      "Epoch   323 , batch Loss= 0.052802, Training Accuracy= 0.99000\n",
      "Epoch   324 , batch Loss= 0.083670, Training Accuracy= 0.97000\n",
      "Epoch   325 , batch Loss= 0.084877, Training Accuracy= 0.96000\n",
      "Epoch   326 , batch Loss= 0.049956, Training Accuracy= 0.98000\n",
      "Epoch   327 , batch Loss= 0.038610, Training Accuracy= 1.00000\n",
      "Epoch   328 , batch Loss= 0.089395, Training Accuracy= 0.97000\n",
      "Epoch   329 , batch Loss= 0.097497, Training Accuracy= 0.96000\n",
      "Epoch   330 , batch Loss= 0.047898, Training Accuracy= 0.99000\n",
      "Epoch   331 , batch Loss= 0.028517, Training Accuracy= 0.99000\n",
      "Epoch   332 , batch Loss= 0.027989, Training Accuracy= 0.99000\n",
      "Epoch   333 , batch Loss= 0.034144, Training Accuracy= 0.99000\n",
      "Epoch   334 , batch Loss= 0.059694, Training Accuracy= 0.98000\n",
      "Epoch   335 , batch Loss= 0.042060, Training Accuracy= 0.99000\n",
      "Epoch   336 , batch Loss= 0.082163, Training Accuracy= 0.99000\n",
      "Epoch   337 , batch Loss= 0.076364, Training Accuracy= 0.97000\n",
      "Epoch   338 , batch Loss= 0.132185, Training Accuracy= 0.97000\n",
      "Epoch   339 , batch Loss= 0.010905, Training Accuracy= 1.00000\n",
      "Epoch   340 , batch Loss= 0.049415, Training Accuracy= 0.98000\n",
      "Epoch   341 , batch Loss= 0.039442, Training Accuracy= 0.99000\n",
      "Epoch   342 , batch Loss= 0.076868, Training Accuracy= 0.98000\n",
      "Epoch   343 , batch Loss= 0.026251, Training Accuracy= 1.00000\n",
      "Epoch   344 , batch Loss= 0.159323, Training Accuracy= 0.94000\n",
      "Epoch   345 , batch Loss= 0.096826, Training Accuracy= 0.97000\n",
      "Epoch   346 , batch Loss= 0.016104, Training Accuracy= 1.00000\n",
      "Epoch   347 , batch Loss= 0.094027, Training Accuracy= 0.97000\n",
      "Epoch   348 , batch Loss= 0.051812, Training Accuracy= 0.98000\n",
      "Epoch   349 , batch Loss= 0.115309, Training Accuracy= 0.95000\n",
      "Epoch   350 , batch Loss= 0.049835, Training Accuracy= 0.99000\n",
      "Epoch   351 , batch Loss= 0.027453, Training Accuracy= 1.00000\n",
      "Epoch   352 , batch Loss= 0.058033, Training Accuracy= 0.99000\n",
      "Epoch   353 , batch Loss= 0.076494, Training Accuracy= 0.96000\n",
      "Epoch   354 , batch Loss= 0.135880, Training Accuracy= 0.97000\n",
      "Epoch   355 , batch Loss= 0.012541, Training Accuracy= 1.00000\n",
      "Epoch   356 , batch Loss= 0.028751, Training Accuracy= 1.00000\n",
      "Epoch   357 , batch Loss= 0.039566, Training Accuracy= 0.99000\n",
      "Epoch   358 , batch Loss= 0.055333, Training Accuracy= 0.99000\n",
      "Epoch   359 , batch Loss= 0.035524, Training Accuracy= 0.99000\n",
      "Epoch   360 , batch Loss= 0.080171, Training Accuracy= 0.98000\n",
      "Epoch   361 , batch Loss= 0.044166, Training Accuracy= 0.98000\n",
      "Epoch   362 , batch Loss= 0.057796, Training Accuracy= 0.97000\n",
      "Epoch   363 , batch Loss= 0.143168, Training Accuracy= 0.95000\n",
      "Epoch   364 , batch Loss= 0.129453, Training Accuracy= 0.97000\n",
      "Epoch   365 , batch Loss= 0.052351, Training Accuracy= 0.98000\n",
      "Epoch   366 , batch Loss= 0.112412, Training Accuracy= 0.98000\n",
      "Epoch   367 , batch Loss= 0.044387, Training Accuracy= 0.99000\n",
      "Epoch   368 , batch Loss= 0.033004, Training Accuracy= 0.99000\n",
      "Epoch   369 , batch Loss= 0.069208, Training Accuracy= 0.98000\n",
      "Epoch   370 , batch Loss= 0.070323, Training Accuracy= 0.98000\n",
      "Epoch   371 , batch Loss= 0.067252, Training Accuracy= 0.98000\n",
      "Epoch   372 , batch Loss= 0.093043, Training Accuracy= 0.95000\n",
      "Epoch   373 , batch Loss= 0.034467, Training Accuracy= 0.99000\n",
      "Epoch   374 , batch Loss= 0.078935, Training Accuracy= 0.97000\n",
      "Epoch   375 , batch Loss= 0.108261, Training Accuracy= 0.96000\n",
      "Epoch   376 , batch Loss= 0.102845, Training Accuracy= 0.98000\n",
      "Epoch   377 , batch Loss= 0.036936, Training Accuracy= 0.99000\n",
      "Epoch   378 , batch Loss= 0.067815, Training Accuracy= 0.98000\n",
      "Epoch   379 , batch Loss= 0.134689, Training Accuracy= 0.97000\n",
      "Epoch   380 , batch Loss= 0.076554, Training Accuracy= 0.99000\n",
      "Epoch   381 , batch Loss= 0.066435, Training Accuracy= 0.97000\n",
      "Epoch   382 , batch Loss= 0.068570, Training Accuracy= 0.98000\n",
      "Epoch   383 , batch Loss= 0.036408, Training Accuracy= 0.99000\n",
      "Epoch   384 , batch Loss= 0.012142, Training Accuracy= 1.00000\n",
      "Epoch   385 , batch Loss= 0.071840, Training Accuracy= 0.99000\n",
      "Epoch   386 , batch Loss= 0.066034, Training Accuracy= 0.98000\n",
      "Epoch   387 , batch Loss= 0.105680, Training Accuracy= 0.97000\n",
      "Epoch   388 , batch Loss= 0.056743, Training Accuracy= 0.99000\n",
      "Epoch   389 , batch Loss= 0.074338, Training Accuracy= 0.98000\n",
      "Epoch   390 , batch Loss= 0.072387, Training Accuracy= 0.98000\n",
      "Epoch   391 , batch Loss= 0.027684, Training Accuracy= 1.00000\n",
      "Epoch   392 , batch Loss= 0.058008, Training Accuracy= 0.98000\n",
      "Epoch   393 , batch Loss= 0.055099, Training Accuracy= 0.98000\n",
      "Epoch   394 , batch Loss= 0.048497, Training Accuracy= 0.99000\n",
      "Epoch   395 , batch Loss= 0.129709, Training Accuracy= 0.96000\n",
      "Epoch   396 , batch Loss= 0.014684, Training Accuracy= 1.00000\n",
      "Epoch   397 , batch Loss= 0.024168, Training Accuracy= 0.98000\n",
      "Epoch   398 , batch Loss= 0.044804, Training Accuracy= 0.99000\n",
      "Epoch   399 , batch Loss= 0.053378, Training Accuracy= 0.99000\n",
      "Epoch   400 , batch Loss= 0.058599, Training Accuracy= 0.99000\n",
      "Epoch   401 , batch Loss= 0.047692, Training Accuracy= 0.98000\n",
      "Epoch   402 , batch Loss= 0.134933, Training Accuracy= 0.97000\n",
      "Epoch   403 , batch Loss= 0.029528, Training Accuracy= 1.00000\n",
      "Epoch   404 , batch Loss= 0.066170, Training Accuracy= 0.98000\n",
      "Epoch   405 , batch Loss= 0.074247, Training Accuracy= 0.97000\n",
      "Epoch   406 , batch Loss= 0.122555, Training Accuracy= 0.95000\n",
      "Epoch   407 , batch Loss= 0.034153, Training Accuracy= 1.00000\n",
      "Epoch   408 , batch Loss= 0.053828, Training Accuracy= 0.98000\n",
      "Epoch   409 , batch Loss= 0.143524, Training Accuracy= 0.94000\n",
      "Epoch   410 , batch Loss= 0.071302, Training Accuracy= 0.97000\n",
      "Epoch   411 , batch Loss= 0.099123, Training Accuracy= 0.97000\n",
      "Epoch   412 , batch Loss= 0.034173, Training Accuracy= 0.99000\n",
      "Epoch   413 , batch Loss= 0.161725, Training Accuracy= 0.92000\n",
      "Epoch   414 , batch Loss= 0.051300, Training Accuracy= 0.98000\n",
      "Epoch   415 , batch Loss= 0.082346, Training Accuracy= 0.97000\n",
      "Epoch   416 , batch Loss= 0.039722, Training Accuracy= 0.98000\n",
      "Epoch   417 , batch Loss= 0.015696, Training Accuracy= 1.00000\n",
      "Epoch   418 , batch Loss= 0.093464, Training Accuracy= 0.98000\n",
      "Epoch   419 , batch Loss= 0.056244, Training Accuracy= 0.98000\n",
      "Epoch   420 , batch Loss= 0.037161, Training Accuracy= 0.98000\n",
      "Epoch   421 , batch Loss= 0.059653, Training Accuracy= 0.98000\n",
      "Epoch   422 , batch Loss= 0.065565, Training Accuracy= 0.99000\n",
      "Epoch   423 , batch Loss= 0.147223, Training Accuracy= 0.94000\n",
      "Epoch   424 , batch Loss= 0.100012, Training Accuracy= 0.96000\n",
      "Epoch   425 , batch Loss= 0.139091, Training Accuracy= 0.96000\n",
      "Epoch   426 , batch Loss= 0.058733, Training Accuracy= 0.99000\n",
      "Epoch   427 , batch Loss= 0.064547, Training Accuracy= 0.99000\n",
      "Epoch   428 , batch Loss= 0.053240, Training Accuracy= 0.98000\n",
      "Epoch   429 , batch Loss= 0.048140, Training Accuracy= 0.97000\n",
      "Epoch   430 , batch Loss= 0.074400, Training Accuracy= 0.98000\n",
      "Epoch   431 , batch Loss= 0.073702, Training Accuracy= 0.98000\n",
      "Epoch   432 , batch Loss= 0.032231, Training Accuracy= 1.00000\n",
      "Epoch   433 , batch Loss= 0.062078, Training Accuracy= 0.96000\n",
      "Epoch   434 , batch Loss= 0.048769, Training Accuracy= 0.99000\n",
      "Epoch   435 , batch Loss= 0.022541, Training Accuracy= 0.99000\n",
      "Epoch   436 , batch Loss= 0.025793, Training Accuracy= 1.00000\n",
      "Epoch   437 , batch Loss= 0.043837, Training Accuracy= 0.98000\n",
      "Epoch   438 , batch Loss= 0.008907, Training Accuracy= 1.00000\n",
      "Epoch   439 , batch Loss= 0.018128, Training Accuracy= 1.00000\n",
      "Epoch   440 , batch Loss= 0.110865, Training Accuracy= 0.96000\n",
      "Epoch   441 , batch Loss= 0.111102, Training Accuracy= 0.97000\n",
      "Epoch   442 , batch Loss= 0.025862, Training Accuracy= 0.99000\n",
      "Epoch   443 , batch Loss= 0.085672, Training Accuracy= 0.99000\n",
      "Epoch   444 , batch Loss= 0.016078, Training Accuracy= 1.00000\n",
      "Epoch   445 , batch Loss= 0.070258, Training Accuracy= 0.98000\n",
      "Epoch   446 , batch Loss= 0.110625, Training Accuracy= 0.96000\n",
      "Epoch   447 , batch Loss= 0.071473, Training Accuracy= 0.98000\n",
      "Epoch   448 , batch Loss= 0.027196, Training Accuracy= 0.99000\n",
      "Epoch   449 , batch Loss= 0.112338, Training Accuracy= 0.95000\n",
      "Epoch   450 , batch Loss= 0.044900, Training Accuracy= 0.98000\n",
      "Epoch   451 , batch Loss= 0.026240, Training Accuracy= 1.00000\n",
      "Epoch   452 , batch Loss= 0.030165, Training Accuracy= 0.99000\n",
      "Epoch   453 , batch Loss= 0.097930, Training Accuracy= 0.96000\n",
      "Epoch   454 , batch Loss= 0.063928, Training Accuracy= 0.96000\n",
      "Epoch   455 , batch Loss= 0.098004, Training Accuracy= 0.97000\n",
      "Epoch   456 , batch Loss= 0.106351, Training Accuracy= 0.98000\n",
      "Epoch   457 , batch Loss= 0.030644, Training Accuracy= 0.99000\n",
      "Epoch   458 , batch Loss= 0.095031, Training Accuracy= 0.97000\n",
      "Epoch   459 , batch Loss= 0.060732, Training Accuracy= 0.98000\n",
      "Epoch   460 , batch Loss= 0.021650, Training Accuracy= 1.00000\n",
      "Epoch   461 , batch Loss= 0.015822, Training Accuracy= 0.99000\n",
      "Epoch   462 , batch Loss= 0.033186, Training Accuracy= 0.98000\n",
      "Epoch   463 , batch Loss= 0.163584, Training Accuracy= 0.95000\n",
      "Epoch   464 , batch Loss= 0.054516, Training Accuracy= 0.98000\n",
      "Epoch   465 , batch Loss= 0.027449, Training Accuracy= 0.98000\n",
      "Epoch   466 , batch Loss= 0.066605, Training Accuracy= 0.98000\n",
      "Epoch   467 , batch Loss= 0.067788, Training Accuracy= 0.97000\n",
      "Epoch   468 , batch Loss= 0.089078, Training Accuracy= 0.98000\n",
      "Epoch   469 , batch Loss= 0.005645, Training Accuracy= 1.00000\n",
      "Epoch   470 , batch Loss= 0.102130, Training Accuracy= 0.94000\n",
      "Epoch   471 , batch Loss= 0.042246, Training Accuracy= 0.99000\n",
      "Epoch   472 , batch Loss= 0.054963, Training Accuracy= 0.98000\n",
      "Epoch   473 , batch Loss= 0.091175, Training Accuracy= 0.97000\n",
      "Epoch   474 , batch Loss= 0.039244, Training Accuracy= 0.99000\n",
      "Epoch   475 , batch Loss= 0.043547, Training Accuracy= 0.99000\n",
      "Epoch   476 , batch Loss= 0.011659, Training Accuracy= 1.00000\n",
      "Epoch   477 , batch Loss= 0.047699, Training Accuracy= 0.99000\n",
      "Epoch   478 , batch Loss= 0.060604, Training Accuracy= 0.98000\n",
      "Epoch   479 , batch Loss= 0.144702, Training Accuracy= 0.95000\n",
      "Epoch   480 , batch Loss= 0.196076, Training Accuracy= 0.95000\n",
      "Epoch   481 , batch Loss= 0.033909, Training Accuracy= 0.99000\n",
      "Epoch   482 , batch Loss= 0.044901, Training Accuracy= 0.98000\n",
      "Epoch   483 , batch Loss= 0.074107, Training Accuracy= 0.99000\n",
      "Epoch   484 , batch Loss= 0.039438, Training Accuracy= 0.98000\n",
      "Epoch   485 , batch Loss= 0.020792, Training Accuracy= 0.99000\n",
      "Epoch   486 , batch Loss= 0.059116, Training Accuracy= 0.98000\n",
      "Epoch   487 , batch Loss= 0.095284, Training Accuracy= 0.96000\n",
      "Epoch   488 , batch Loss= 0.031992, Training Accuracy= 0.99000\n",
      "Epoch   489 , batch Loss= 0.087079, Training Accuracy= 0.97000\n",
      "Epoch   490 , batch Loss= 0.047371, Training Accuracy= 0.99000\n",
      "Epoch   491 , batch Loss= 0.074971, Training Accuracy= 0.98000\n",
      "Epoch   492 , batch Loss= 0.016821, Training Accuracy= 0.99000\n",
      "Epoch   493 , batch Loss= 0.035497, Training Accuracy= 0.99000\n",
      "Epoch   494 , batch Loss= 0.014936, Training Accuracy= 1.00000\n",
      "Epoch   495 , batch Loss= 0.014727, Training Accuracy= 1.00000\n",
      "Epoch   496 , batch Loss= 0.103102, Training Accuracy= 0.98000\n",
      "Epoch   497 , batch Loss= 0.015185, Training Accuracy= 1.00000\n",
      "Epoch   498 , batch Loss= 0.069719, Training Accuracy= 0.98000\n",
      "Epoch   499 , batch Loss= 0.046419, Training Accuracy= 0.99000\n",
      "Epoch   500 , batch Loss= 0.039912, Training Accuracy= 0.99000\n",
      "Epoch   501 , batch Loss= 0.014706, Training Accuracy= 1.00000\n",
      "Epoch   502 , batch Loss= 0.009048, Training Accuracy= 1.00000\n",
      "Epoch   503 , batch Loss= 0.024982, Training Accuracy= 1.00000\n",
      "Epoch   504 , batch Loss= 0.085588, Training Accuracy= 0.97000\n",
      "Epoch   505 , batch Loss= 0.014350, Training Accuracy= 1.00000\n",
      "Epoch   506 , batch Loss= 0.038019, Training Accuracy= 0.98000\n",
      "Epoch   507 , batch Loss= 0.025774, Training Accuracy= 1.00000\n",
      "Epoch   508 , batch Loss= 0.079792, Training Accuracy= 0.97000\n",
      "Epoch   509 , batch Loss= 0.048818, Training Accuracy= 0.98000\n",
      "Epoch   510 , batch Loss= 0.030853, Training Accuracy= 0.98000\n",
      "Epoch   511 , batch Loss= 0.031041, Training Accuracy= 0.99000\n",
      "Epoch   512 , batch Loss= 0.011554, Training Accuracy= 1.00000\n",
      "Epoch   513 , batch Loss= 0.042214, Training Accuracy= 0.99000\n",
      "Epoch   514 , batch Loss= 0.034122, Training Accuracy= 0.98000\n",
      "Epoch   515 , batch Loss= 0.069309, Training Accuracy= 0.98000\n",
      "Epoch   516 , batch Loss= 0.045909, Training Accuracy= 0.99000\n",
      "Epoch   517 , batch Loss= 0.012376, Training Accuracy= 1.00000\n",
      "Epoch   518 , batch Loss= 0.014299, Training Accuracy= 1.00000\n",
      "Epoch   519 , batch Loss= 0.031837, Training Accuracy= 1.00000\n",
      "Epoch   520 , batch Loss= 0.008294, Training Accuracy= 1.00000\n",
      "Epoch   521 , batch Loss= 0.055409, Training Accuracy= 0.97000\n",
      "Epoch   522 , batch Loss= 0.004724, Training Accuracy= 1.00000\n",
      "Epoch   523 , batch Loss= 0.092925, Training Accuracy= 0.98000\n",
      "Epoch   524 , batch Loss= 0.047483, Training Accuracy= 0.99000\n",
      "Epoch   525 , batch Loss= 0.023741, Training Accuracy= 0.99000\n",
      "Epoch   526 , batch Loss= 0.011238, Training Accuracy= 1.00000\n",
      "Epoch   527 , batch Loss= 0.033955, Training Accuracy= 0.99000\n",
      "Epoch   528 , batch Loss= 0.075290, Training Accuracy= 0.99000\n",
      "Epoch   529 , batch Loss= 0.016050, Training Accuracy= 0.99000\n",
      "Epoch   530 , batch Loss= 0.004709, Training Accuracy= 1.00000\n",
      "Epoch   531 , batch Loss= 0.061297, Training Accuracy= 0.98000\n",
      "Epoch   532 , batch Loss= 0.013000, Training Accuracy= 1.00000\n",
      "Epoch   533 , batch Loss= 0.007907, Training Accuracy= 0.99000\n",
      "Epoch   534 , batch Loss= 0.014261, Training Accuracy= 0.99000\n",
      "Epoch   535 , batch Loss= 0.030118, Training Accuracy= 0.99000\n",
      "Epoch   536 , batch Loss= 0.016461, Training Accuracy= 1.00000\n",
      "Epoch   537 , batch Loss= 0.013879, Training Accuracy= 1.00000\n",
      "Epoch   538 , batch Loss= 0.001545, Training Accuracy= 1.00000\n",
      "Epoch   539 , batch Loss= 0.045758, Training Accuracy= 0.99000\n",
      "Epoch   540 , batch Loss= 0.003437, Training Accuracy= 1.00000\n",
      "Epoch   541 , batch Loss= 0.000594, Training Accuracy= 1.00000\n",
      "Epoch   542 , batch Loss= 0.000354, Training Accuracy= 1.00000\n",
      "Epoch   543 , batch Loss= 0.012065, Training Accuracy= 1.00000\n",
      "Epoch   544 , batch Loss= 0.065790, Training Accuracy= 0.98000\n",
      "Epoch   545 , batch Loss= 0.016607, Training Accuracy= 1.00000\n",
      "Epoch   546 , batch Loss= 0.001944, Training Accuracy= 1.00000\n",
      "Epoch   547 , batch Loss= 0.019853, Training Accuracy= 0.99000\n",
      "Epoch   548 , batch Loss= 0.249246, Training Accuracy= 0.95000\n",
      "Epoch   549 , batch Loss= 0.004377, Training Accuracy= 1.00000\n",
      "Epoch   550 , batch Loss= 0.232124, Training Accuracy= 0.99000\n",
      "Epoch   551 , batch Loss= 0.028294, Training Accuracy= 0.99000\n",
      "Epoch   552 , batch Loss= 0.029270, Training Accuracy= 0.98000\n",
      "Epoch   553 , batch Loss= 0.070269, Training Accuracy= 0.96000\n",
      "Epoch   554 , batch Loss= 0.013876, Training Accuracy= 0.99000\n",
      "Epoch   555 , batch Loss= 0.017030, Training Accuracy= 0.99000\n",
      "Epoch   556 , batch Loss= 0.013541, Training Accuracy= 1.00000\n",
      "Epoch   557 , batch Loss= 0.010696, Training Accuracy= 1.00000\n",
      "Epoch   558 , batch Loss= 0.079887, Training Accuracy= 0.98000\n",
      "Epoch   559 , batch Loss= 0.002923, Training Accuracy= 1.00000\n",
      "Epoch   560 , batch Loss= 0.073772, Training Accuracy= 0.98000\n",
      "Epoch   561 , batch Loss= 0.015561, Training Accuracy= 1.00000\n",
      "Epoch   562 , batch Loss= 0.086850, Training Accuracy= 0.99000\n",
      "Epoch   563 , batch Loss= 0.024441, Training Accuracy= 0.98000\n",
      "Epoch   564 , batch Loss= 0.018069, Training Accuracy= 0.99000\n",
      "Epoch   565 , batch Loss= 0.031748, Training Accuracy= 0.98000\n",
      "Epoch   566 , batch Loss= 0.023658, Training Accuracy= 0.99000\n",
      "Epoch   567 , batch Loss= 0.010341, Training Accuracy= 1.00000\n",
      "Epoch   568 , batch Loss= 0.011085, Training Accuracy= 1.00000\n",
      "Epoch   569 , batch Loss= 0.004475, Training Accuracy= 1.00000\n",
      "Epoch   570 , batch Loss= 0.027141, Training Accuracy= 0.99000\n",
      "Epoch   571 , batch Loss= 0.022419, Training Accuracy= 1.00000\n",
      "Epoch   572 , batch Loss= 0.051242, Training Accuracy= 0.99000\n",
      "Epoch   573 , batch Loss= 0.053880, Training Accuracy= 0.97000\n",
      "Epoch   574 , batch Loss= 0.011923, Training Accuracy= 1.00000\n",
      "Epoch   575 , batch Loss= 0.078047, Training Accuracy= 0.98000\n",
      "Epoch   576 , batch Loss= 0.078865, Training Accuracy= 0.98000\n",
      "Epoch   577 , batch Loss= 0.053964, Training Accuracy= 0.98000\n",
      "Epoch   578 , batch Loss= 0.016128, Training Accuracy= 1.00000\n",
      "Epoch   579 , batch Loss= 0.005919, Training Accuracy= 1.00000\n",
      "Epoch   580 , batch Loss= 0.019595, Training Accuracy= 1.00000\n",
      "Epoch   581 , batch Loss= 0.026379, Training Accuracy= 0.98000\n",
      "Epoch   582 , batch Loss= 0.014805, Training Accuracy= 0.99000\n",
      "Epoch   583 , batch Loss= 0.041169, Training Accuracy= 0.99000\n",
      "Epoch   584 , batch Loss= 0.012004, Training Accuracy= 1.00000\n",
      "Epoch   585 , batch Loss= 0.016900, Training Accuracy= 0.99000\n",
      "Epoch   586 , batch Loss= 0.050211, Training Accuracy= 0.99000\n",
      "Epoch   587 , batch Loss= 0.030648, Training Accuracy= 1.00000\n",
      "Epoch   588 , batch Loss= 0.032091, Training Accuracy= 0.98000\n",
      "Epoch   589 , batch Loss= 0.063094, Training Accuracy= 0.97000\n",
      "Epoch   590 , batch Loss= 0.031838, Training Accuracy= 0.99000\n",
      "Epoch   591 , batch Loss= 0.017610, Training Accuracy= 1.00000\n",
      "Epoch   592 , batch Loss= 0.064465, Training Accuracy= 0.97000\n",
      "Epoch   593 , batch Loss= 0.042599, Training Accuracy= 0.98000\n",
      "Epoch   594 , batch Loss= 0.063336, Training Accuracy= 0.98000\n",
      "Epoch   595 , batch Loss= 0.012366, Training Accuracy= 1.00000\n",
      "Epoch   596 , batch Loss= 0.020992, Training Accuracy= 0.99000\n",
      "Epoch   597 , batch Loss= 0.029244, Training Accuracy= 0.99000\n",
      "Epoch   598 , batch Loss= 0.022390, Training Accuracy= 1.00000\n",
      "Epoch   599 , batch Loss= 0.011829, Training Accuracy= 1.00000\n",
      "Epoch   600 , batch Loss= 0.014611, Training Accuracy= 1.00000\n",
      "Epoch   601 , batch Loss= 0.018875, Training Accuracy= 1.00000\n",
      "Epoch   602 , batch Loss= 0.018222, Training Accuracy= 0.99000\n",
      "Epoch   603 , batch Loss= 0.002945, Training Accuracy= 1.00000\n",
      "Epoch   604 , batch Loss= 0.013546, Training Accuracy= 1.00000\n",
      "Epoch   605 , batch Loss= 0.008791, Training Accuracy= 1.00000\n",
      "Epoch   606 , batch Loss= 0.054411, Training Accuracy= 0.99000\n",
      "Epoch   607 , batch Loss= 0.037470, Training Accuracy= 0.99000\n",
      "Epoch   608 , batch Loss= 0.008708, Training Accuracy= 1.00000\n",
      "Epoch   609 , batch Loss= 0.040753, Training Accuracy= 0.98000\n",
      "Epoch   610 , batch Loss= 0.021943, Training Accuracy= 0.99000\n",
      "Epoch   611 , batch Loss= 0.027137, Training Accuracy= 0.99000\n",
      "Epoch   612 , batch Loss= 0.064098, Training Accuracy= 0.99000\n",
      "Epoch   613 , batch Loss= 0.010682, Training Accuracy= 1.00000\n",
      "Epoch   614 , batch Loss= 0.005211, Training Accuracy= 1.00000\n",
      "Epoch   615 , batch Loss= 0.015770, Training Accuracy= 0.99000\n",
      "Epoch   616 , batch Loss= 0.007322, Training Accuracy= 1.00000\n",
      "Epoch   617 , batch Loss= 0.034191, Training Accuracy= 0.99000\n",
      "Epoch   618 , batch Loss= 0.017186, Training Accuracy= 1.00000\n",
      "Epoch   619 , batch Loss= 0.016936, Training Accuracy= 0.99000\n",
      "Epoch   620 , batch Loss= 0.007946, Training Accuracy= 1.00000\n",
      "Epoch   621 , batch Loss= 0.011194, Training Accuracy= 1.00000\n",
      "Epoch   622 , batch Loss= 0.052126, Training Accuracy= 0.98000\n",
      "Epoch   623 , batch Loss= 0.053929, Training Accuracy= 0.99000\n",
      "Epoch   624 , batch Loss= 0.018363, Training Accuracy= 0.99000\n",
      "Epoch   625 , batch Loss= 0.005447, Training Accuracy= 1.00000\n",
      "Epoch   626 , batch Loss= 0.004935, Training Accuracy= 1.00000\n",
      "Epoch   627 , batch Loss= 0.027114, Training Accuracy= 0.98000\n",
      "Epoch   628 , batch Loss= 0.036623, Training Accuracy= 0.99000\n",
      "Epoch   629 , batch Loss= 0.008087, Training Accuracy= 1.00000\n",
      "Epoch   630 , batch Loss= 0.014136, Training Accuracy= 0.99000\n",
      "Epoch   631 , batch Loss= 0.008367, Training Accuracy= 1.00000\n",
      "Epoch   632 , batch Loss= 0.048354, Training Accuracy= 0.99000\n",
      "Epoch   633 , batch Loss= 0.036061, Training Accuracy= 0.99000\n",
      "Epoch   634 , batch Loss= 0.010671, Training Accuracy= 1.00000\n",
      "Epoch   635 , batch Loss= 0.052862, Training Accuracy= 0.98000\n",
      "Epoch   636 , batch Loss= 0.014624, Training Accuracy= 1.00000\n",
      "Epoch   637 , batch Loss= 0.017085, Training Accuracy= 0.99000\n",
      "Epoch   638 , batch Loss= 0.049934, Training Accuracy= 0.99000\n",
      "Epoch   639 , batch Loss= 0.031900, Training Accuracy= 0.99000\n",
      "Epoch   640 , batch Loss= 0.001679, Training Accuracy= 1.00000\n",
      "Epoch   641 , batch Loss= 0.028692, Training Accuracy= 0.99000\n",
      "Epoch   642 , batch Loss= 0.007720, Training Accuracy= 1.00000\n",
      "Epoch   643 , batch Loss= 0.012850, Training Accuracy= 1.00000\n",
      "Epoch   644 , batch Loss= 0.025717, Training Accuracy= 0.99000\n",
      "Epoch   645 , batch Loss= 0.048035, Training Accuracy= 0.98000\n",
      "Epoch   646 , batch Loss= 0.003526, Training Accuracy= 1.00000\n",
      "Epoch   647 , batch Loss= 0.051244, Training Accuracy= 0.98000\n",
      "Epoch   648 , batch Loss= 0.005951, Training Accuracy= 1.00000\n",
      "Epoch   649 , batch Loss= 0.016857, Training Accuracy= 0.99000\n",
      "Epoch   650 , batch Loss= 0.018876, Training Accuracy= 0.99000\n",
      "Epoch   651 , batch Loss= 0.008183, Training Accuracy= 1.00000\n",
      "Epoch   652 , batch Loss= 0.017010, Training Accuracy= 0.99000\n",
      "Epoch   653 , batch Loss= 0.017900, Training Accuracy= 1.00000\n",
      "Epoch   654 , batch Loss= 0.004093, Training Accuracy= 1.00000\n",
      "Epoch   655 , batch Loss= 0.040692, Training Accuracy= 0.99000\n",
      "Epoch   656 , batch Loss= 0.035655, Training Accuracy= 0.99000\n",
      "Epoch   657 , batch Loss= 0.018199, Training Accuracy= 1.00000\n",
      "Epoch   658 , batch Loss= 0.010609, Training Accuracy= 1.00000\n",
      "Epoch   659 , batch Loss= 0.058408, Training Accuracy= 0.98000\n",
      "Epoch   660 , batch Loss= 0.144312, Training Accuracy= 0.96000\n",
      "Epoch   661 , batch Loss= 0.015956, Training Accuracy= 0.99000\n",
      "Epoch   662 , batch Loss= 0.004546, Training Accuracy= 1.00000\n",
      "Epoch   663 , batch Loss= 0.092846, Training Accuracy= 0.97000\n",
      "Epoch   664 , batch Loss= 0.048059, Training Accuracy= 0.99000\n",
      "Epoch   665 , batch Loss= 0.004980, Training Accuracy= 1.00000\n",
      "Epoch   666 , batch Loss= 0.051788, Training Accuracy= 0.97000\n",
      "Epoch   667 , batch Loss= 0.099455, Training Accuracy= 0.96000\n",
      "Epoch   668 , batch Loss= 0.025474, Training Accuracy= 0.99000\n",
      "Epoch   669 , batch Loss= 0.028033, Training Accuracy= 0.99000\n",
      "Epoch   670 , batch Loss= 0.011023, Training Accuracy= 1.00000\n",
      "Epoch   671 , batch Loss= 0.012378, Training Accuracy= 1.00000\n",
      "Epoch   672 , batch Loss= 0.020521, Training Accuracy= 0.99000\n",
      "Epoch   673 , batch Loss= 0.008674, Training Accuracy= 1.00000\n",
      "Epoch   674 , batch Loss= 0.021074, Training Accuracy= 0.99000\n",
      "Epoch   675 , batch Loss= 0.023605, Training Accuracy= 1.00000\n",
      "Epoch   676 , batch Loss= 0.059459, Training Accuracy= 0.98000\n",
      "Epoch   677 , batch Loss= 0.021135, Training Accuracy= 0.99000\n",
      "Epoch   678 , batch Loss= 0.054051, Training Accuracy= 0.97000\n",
      "Epoch   679 , batch Loss= 0.059423, Training Accuracy= 0.97000\n",
      "Epoch   680 , batch Loss= 0.019267, Training Accuracy= 0.99000\n",
      "Epoch   681 , batch Loss= 0.012220, Training Accuracy= 1.00000\n",
      "Epoch   682 , batch Loss= 0.018423, Training Accuracy= 1.00000\n",
      "Epoch   683 , batch Loss= 0.024994, Training Accuracy= 0.99000\n",
      "Epoch   684 , batch Loss= 0.010598, Training Accuracy= 1.00000\n",
      "Epoch   685 , batch Loss= 0.020966, Training Accuracy= 0.99000\n",
      "Epoch   686 , batch Loss= 0.023747, Training Accuracy= 0.99000\n",
      "Epoch   687 , batch Loss= 0.028231, Training Accuracy= 0.99000\n",
      "Epoch   688 , batch Loss= 0.032785, Training Accuracy= 0.99000\n",
      "Epoch   689 , batch Loss= 0.012172, Training Accuracy= 1.00000\n",
      "Epoch   690 , batch Loss= 0.068576, Training Accuracy= 0.98000\n",
      "Epoch   691 , batch Loss= 0.028100, Training Accuracy= 0.99000\n",
      "Epoch   692 , batch Loss= 0.009113, Training Accuracy= 1.00000\n",
      "Epoch   693 , batch Loss= 0.016491, Training Accuracy= 1.00000\n",
      "Epoch   694 , batch Loss= 0.016088, Training Accuracy= 1.00000\n",
      "Epoch   695 , batch Loss= 0.021447, Training Accuracy= 0.99000\n",
      "Epoch   696 , batch Loss= 0.007254, Training Accuracy= 1.00000\n",
      "Epoch   697 , batch Loss= 0.008513, Training Accuracy= 1.00000\n",
      "Epoch   698 , batch Loss= 0.004701, Training Accuracy= 1.00000\n",
      "Epoch   699 , batch Loss= 0.024367, Training Accuracy= 0.98000\n",
      "Epoch   700 , batch Loss= 0.004825, Training Accuracy= 1.00000\n",
      "Epoch   701 , batch Loss= 0.094475, Training Accuracy= 0.98000\n",
      "Epoch   702 , batch Loss= 0.035244, Training Accuracy= 0.98000\n",
      "Epoch   703 , batch Loss= 0.015990, Training Accuracy= 0.99000\n",
      "Epoch   704 , batch Loss= 0.043539, Training Accuracy= 0.99000\n",
      "Epoch   705 , batch Loss= 0.022892, Training Accuracy= 0.99000\n",
      "Epoch   706 , batch Loss= 0.009861, Training Accuracy= 1.00000\n",
      "Epoch   707 , batch Loss= 0.027125, Training Accuracy= 0.98000\n",
      "Epoch   708 , batch Loss= 0.031073, Training Accuracy= 0.99000\n",
      "Epoch   709 , batch Loss= 0.022242, Training Accuracy= 0.99000\n",
      "Epoch   710 , batch Loss= 0.020129, Training Accuracy= 0.99000\n",
      "Epoch   711 , batch Loss= 0.006087, Training Accuracy= 1.00000\n",
      "Epoch   712 , batch Loss= 0.031048, Training Accuracy= 0.98000\n",
      "Epoch   713 , batch Loss= 0.008079, Training Accuracy= 1.00000\n",
      "Epoch   714 , batch Loss= 0.053925, Training Accuracy= 0.97000\n",
      "Epoch   715 , batch Loss= 0.017907, Training Accuracy= 0.99000\n",
      "Epoch   716 , batch Loss= 0.015488, Training Accuracy= 0.99000\n",
      "Epoch   717 , batch Loss= 0.022148, Training Accuracy= 0.99000\n",
      "Epoch   718 , batch Loss= 0.005605, Training Accuracy= 1.00000\n",
      "Epoch   719 , batch Loss= 0.020342, Training Accuracy= 1.00000\n",
      "Epoch   720 , batch Loss= 0.036895, Training Accuracy= 0.99000\n",
      "Epoch   721 , batch Loss= 0.011740, Training Accuracy= 1.00000\n",
      "Epoch   722 , batch Loss= 0.106906, Training Accuracy= 0.97000\n",
      "Epoch   723 , batch Loss= 0.014909, Training Accuracy= 1.00000\n",
      "Epoch   724 , batch Loss= 0.020782, Training Accuracy= 1.00000\n",
      "Epoch   725 , batch Loss= 0.003033, Training Accuracy= 1.00000\n",
      "Epoch   726 , batch Loss= 0.010402, Training Accuracy= 1.00000\n",
      "Epoch   727 , batch Loss= 0.008280, Training Accuracy= 1.00000\n",
      "Epoch   728 , batch Loss= 0.026999, Training Accuracy= 0.99000\n",
      "Epoch   729 , batch Loss= 0.038111, Training Accuracy= 0.99000\n",
      "Epoch   730 , batch Loss= 0.046987, Training Accuracy= 0.98000\n",
      "Epoch   731 , batch Loss= 0.027343, Training Accuracy= 0.99000\n",
      "Epoch   732 , batch Loss= 0.010611, Training Accuracy= 1.00000\n",
      "Epoch   733 , batch Loss= 0.017282, Training Accuracy= 0.99000\n",
      "Epoch   734 , batch Loss= 0.007984, Training Accuracy= 1.00000\n",
      "Epoch   735 , batch Loss= 0.010040, Training Accuracy= 1.00000\n",
      "Epoch   736 , batch Loss= 0.006794, Training Accuracy= 1.00000\n",
      "Epoch   737 , batch Loss= 0.015978, Training Accuracy= 0.99000\n",
      "Epoch   738 , batch Loss= 0.110409, Training Accuracy= 0.98000\n",
      "Epoch   739 , batch Loss= 0.019470, Training Accuracy= 0.99000\n",
      "Epoch   740 , batch Loss= 0.030732, Training Accuracy= 0.98000\n",
      "Epoch   741 , batch Loss= 0.002449, Training Accuracy= 1.00000\n",
      "Epoch   742 , batch Loss= 0.073277, Training Accuracy= 0.98000\n",
      "Epoch   743 , batch Loss= 0.006146, Training Accuracy= 1.00000\n",
      "Epoch   744 , batch Loss= 0.005589, Training Accuracy= 1.00000\n",
      "Epoch   745 , batch Loss= 0.003096, Training Accuracy= 1.00000\n",
      "Epoch   746 , batch Loss= 0.007207, Training Accuracy= 1.00000\n",
      "Epoch   747 , batch Loss= 0.034815, Training Accuracy= 0.98000\n",
      "Epoch   748 , batch Loss= 0.007463, Training Accuracy= 1.00000\n",
      "Epoch   749 , batch Loss= 0.010262, Training Accuracy= 1.00000\n",
      "Epoch   750 , batch Loss= 0.059244, Training Accuracy= 0.98000\n",
      "Epoch   751 , batch Loss= 0.008744, Training Accuracy= 1.00000\n",
      "Epoch   752 , batch Loss= 0.023583, Training Accuracy= 0.99000\n",
      "Epoch   753 , batch Loss= 0.021123, Training Accuracy= 0.99000\n",
      "Epoch   754 , batch Loss= 0.008207, Training Accuracy= 1.00000\n",
      "Epoch   755 , batch Loss= 0.007636, Training Accuracy= 1.00000\n",
      "Epoch   756 , batch Loss= 0.024755, Training Accuracy= 0.99000\n",
      "Epoch   757 , batch Loss= 0.015758, Training Accuracy= 1.00000\n",
      "Epoch   758 , batch Loss= 0.006595, Training Accuracy= 1.00000\n",
      "Epoch   759 , batch Loss= 0.034822, Training Accuracy= 0.98000\n",
      "Epoch   760 , batch Loss= 0.028095, Training Accuracy= 0.99000\n",
      "Epoch   761 , batch Loss= 0.069170, Training Accuracy= 0.98000\n",
      "Epoch   762 , batch Loss= 0.063200, Training Accuracy= 0.98000\n",
      "Epoch   763 , batch Loss= 0.008044, Training Accuracy= 1.00000\n",
      "Epoch   764 , batch Loss= 0.020609, Training Accuracy= 0.99000\n",
      "Epoch   765 , batch Loss= 0.121883, Training Accuracy= 0.97000\n",
      "Epoch   766 , batch Loss= 0.004650, Training Accuracy= 1.00000\n",
      "Epoch   767 , batch Loss= 0.033492, Training Accuracy= 0.98000\n",
      "Epoch   768 , batch Loss= 0.007856, Training Accuracy= 1.00000\n",
      "Epoch   769 , batch Loss= 0.022864, Training Accuracy= 0.99000\n",
      "Epoch   770 , batch Loss= 0.032747, Training Accuracy= 0.99000\n",
      "Epoch   771 , batch Loss= 0.021677, Training Accuracy= 0.99000\n",
      "Epoch   772 , batch Loss= 0.007174, Training Accuracy= 1.00000\n",
      "Epoch   773 , batch Loss= 0.012191, Training Accuracy= 1.00000\n",
      "Epoch   774 , batch Loss= 0.017959, Training Accuracy= 1.00000\n",
      "Epoch   775 , batch Loss= 0.006543, Training Accuracy= 1.00000\n",
      "Epoch   776 , batch Loss= 0.018103, Training Accuracy= 1.00000\n",
      "Epoch   777 , batch Loss= 0.035112, Training Accuracy= 1.00000\n",
      "Epoch   778 , batch Loss= 0.005984, Training Accuracy= 1.00000\n",
      "Epoch   779 , batch Loss= 0.032170, Training Accuracy= 0.98000\n",
      "Epoch   780 , batch Loss= 0.005992, Training Accuracy= 1.00000\n",
      "Epoch   781 , batch Loss= 0.030759, Training Accuracy= 0.99000\n",
      "Epoch   782 , batch Loss= 0.048343, Training Accuracy= 0.99000\n",
      "Epoch   783 , batch Loss= 0.031161, Training Accuracy= 0.99000\n",
      "Epoch   784 , batch Loss= 0.005819, Training Accuracy= 1.00000\n",
      "Epoch   785 , batch Loss= 0.022765, Training Accuracy= 0.99000\n",
      "Epoch   786 , batch Loss= 0.048740, Training Accuracy= 0.99000\n",
      "Epoch   787 , batch Loss= 0.044749, Training Accuracy= 0.99000\n",
      "Epoch   788 , batch Loss= 0.005976, Training Accuracy= 1.00000\n",
      "Epoch   789 , batch Loss= 0.012791, Training Accuracy= 1.00000\n",
      "Epoch   790 , batch Loss= 0.003890, Training Accuracy= 1.00000\n",
      "Epoch   791 , batch Loss= 0.024955, Training Accuracy= 0.99000\n",
      "Epoch   792 , batch Loss= 0.034063, Training Accuracy= 0.98000\n",
      "Epoch   793 , batch Loss= 0.038655, Training Accuracy= 0.99000\n",
      "Epoch   794 , batch Loss= 0.043177, Training Accuracy= 0.97000\n",
      "Epoch   795 , batch Loss= 0.019686, Training Accuracy= 0.99000\n",
      "Epoch   796 , batch Loss= 0.012499, Training Accuracy= 1.00000\n",
      "Epoch   797 , batch Loss= 0.017546, Training Accuracy= 1.00000\n",
      "Epoch   798 , batch Loss= 0.073798, Training Accuracy= 0.98000\n",
      "Epoch   799 , batch Loss= 0.013140, Training Accuracy= 1.00000\n",
      "Epoch   800 , batch Loss= 0.029834, Training Accuracy= 0.99000\n",
      "Epoch   801 , batch Loss= 0.019357, Training Accuracy= 0.99000\n",
      "Epoch   802 , batch Loss= 0.011078, Training Accuracy= 1.00000\n",
      "Epoch   803 , batch Loss= 0.080378, Training Accuracy= 0.98000\n",
      "Epoch   804 , batch Loss= 0.016387, Training Accuracy= 1.00000\n",
      "Epoch   805 , batch Loss= 0.033762, Training Accuracy= 0.99000\n",
      "Epoch   806 , batch Loss= 0.030612, Training Accuracy= 0.99000\n",
      "Epoch   807 , batch Loss= 0.041865, Training Accuracy= 0.99000\n",
      "Epoch   808 , batch Loss= 0.022611, Training Accuracy= 0.99000\n",
      "Epoch   809 , batch Loss= 0.037984, Training Accuracy= 0.98000\n",
      "Epoch   810 , batch Loss= 0.069789, Training Accuracy= 0.99000\n",
      "Epoch   811 , batch Loss= 0.011413, Training Accuracy= 0.99000\n",
      "Epoch   812 , batch Loss= 0.002129, Training Accuracy= 1.00000\n",
      "Epoch   813 , batch Loss= 0.028074, Training Accuracy= 0.99000\n",
      "Epoch   814 , batch Loss= 0.039590, Training Accuracy= 0.99000\n",
      "Epoch   815 , batch Loss= 0.008344, Training Accuracy= 1.00000\n",
      "Epoch   816 , batch Loss= 0.116917, Training Accuracy= 0.98000\n",
      "Epoch   817 , batch Loss= 0.004031, Training Accuracy= 1.00000\n",
      "Epoch   818 , batch Loss= 0.047842, Training Accuracy= 0.99000\n",
      "Epoch   819 , batch Loss= 0.038074, Training Accuracy= 0.99000\n",
      "Epoch   820 , batch Loss= 0.015497, Training Accuracy= 1.00000\n",
      "Epoch   821 , batch Loss= 0.048201, Training Accuracy= 0.99000\n",
      "Epoch   822 , batch Loss= 0.007765, Training Accuracy= 1.00000\n",
      "Epoch   823 , batch Loss= 0.068235, Training Accuracy= 0.99000\n",
      "Epoch   824 , batch Loss= 0.014350, Training Accuracy= 1.00000\n",
      "Epoch   825 , batch Loss= 0.015224, Training Accuracy= 0.99000\n",
      "Epoch   826 , batch Loss= 0.024012, Training Accuracy= 0.98000\n",
      "Epoch   827 , batch Loss= 0.038572, Training Accuracy= 0.99000\n",
      "Epoch   828 , batch Loss= 0.017277, Training Accuracy= 1.00000\n",
      "Epoch   829 , batch Loss= 0.031469, Training Accuracy= 0.99000\n",
      "Epoch   830 , batch Loss= 0.025756, Training Accuracy= 1.00000\n",
      "Epoch   831 , batch Loss= 0.033635, Training Accuracy= 0.99000\n",
      "Epoch   832 , batch Loss= 0.013837, Training Accuracy= 1.00000\n",
      "Epoch   833 , batch Loss= 0.022702, Training Accuracy= 0.99000\n",
      "Epoch   834 , batch Loss= 0.045824, Training Accuracy= 0.98000\n",
      "Epoch   835 , batch Loss= 0.025805, Training Accuracy= 0.99000\n",
      "Epoch   836 , batch Loss= 0.020859, Training Accuracy= 1.00000\n",
      "Epoch   837 , batch Loss= 0.009480, Training Accuracy= 1.00000\n",
      "Epoch   838 , batch Loss= 0.023952, Training Accuracy= 0.99000\n",
      "Epoch   839 , batch Loss= 0.048915, Training Accuracy= 0.99000\n",
      "Epoch   840 , batch Loss= 0.043710, Training Accuracy= 0.98000\n",
      "Epoch   841 , batch Loss= 0.035630, Training Accuracy= 0.99000\n",
      "Epoch   842 , batch Loss= 0.093326, Training Accuracy= 0.96000\n",
      "Epoch   843 , batch Loss= 0.044834, Training Accuracy= 0.98000\n",
      "Epoch   844 , batch Loss= 0.045195, Training Accuracy= 0.99000\n",
      "Epoch   845 , batch Loss= 0.055470, Training Accuracy= 0.98000\n",
      "Epoch   846 , batch Loss= 0.008888, Training Accuracy= 1.00000\n",
      "Epoch   847 , batch Loss= 0.034424, Training Accuracy= 0.99000\n",
      "Epoch   848 , batch Loss= 0.035461, Training Accuracy= 0.98000\n",
      "Epoch   849 , batch Loss= 0.018781, Training Accuracy= 1.00000\n",
      "Epoch   850 , batch Loss= 0.030804, Training Accuracy= 0.99000\n",
      "Epoch   851 , batch Loss= 0.015332, Training Accuracy= 1.00000\n",
      "Epoch   852 , batch Loss= 0.014214, Training Accuracy= 1.00000\n",
      "Epoch   853 , batch Loss= 0.005497, Training Accuracy= 1.00000\n",
      "Epoch   854 , batch Loss= 0.073355, Training Accuracy= 0.99000\n",
      "Epoch   855 , batch Loss= 0.005508, Training Accuracy= 1.00000\n",
      "Epoch   856 , batch Loss= 0.008146, Training Accuracy= 1.00000\n",
      "Epoch   857 , batch Loss= 0.013935, Training Accuracy= 1.00000\n",
      "Epoch   858 , batch Loss= 0.014179, Training Accuracy= 1.00000\n",
      "Epoch   859 , batch Loss= 0.008212, Training Accuracy= 1.00000\n",
      "Epoch   860 , batch Loss= 0.008466, Training Accuracy= 1.00000\n",
      "Epoch   861 , batch Loss= 0.011567, Training Accuracy= 1.00000\n",
      "Epoch   862 , batch Loss= 0.019587, Training Accuracy= 0.99000\n",
      "Epoch   863 , batch Loss= 0.021449, Training Accuracy= 0.99000\n",
      "Epoch   864 , batch Loss= 0.007119, Training Accuracy= 1.00000\n",
      "Epoch   865 , batch Loss= 0.077777, Training Accuracy= 0.98000\n",
      "Epoch   866 , batch Loss= 0.015263, Training Accuracy= 1.00000\n",
      "Epoch   867 , batch Loss= 0.026896, Training Accuracy= 0.98000\n",
      "Epoch   868 , batch Loss= 0.012018, Training Accuracy= 1.00000\n",
      "Epoch   869 , batch Loss= 0.041304, Training Accuracy= 0.97000\n",
      "Epoch   870 , batch Loss= 0.042906, Training Accuracy= 0.99000\n",
      "Epoch   871 , batch Loss= 0.008572, Training Accuracy= 1.00000\n",
      "Epoch   872 , batch Loss= 0.020597, Training Accuracy= 0.99000\n",
      "Epoch   873 , batch Loss= 0.013393, Training Accuracy= 1.00000\n",
      "Epoch   874 , batch Loss= 0.016059, Training Accuracy= 1.00000\n",
      "Epoch   875 , batch Loss= 0.011415, Training Accuracy= 0.99000\n",
      "Epoch   876 , batch Loss= 0.012510, Training Accuracy= 1.00000\n",
      "Epoch   877 , batch Loss= 0.072587, Training Accuracy= 0.97000\n",
      "Epoch   878 , batch Loss= 0.017232, Training Accuracy= 0.99000\n",
      "Epoch   879 , batch Loss= 0.038968, Training Accuracy= 0.99000\n",
      "Epoch   880 , batch Loss= 0.012685, Training Accuracy= 1.00000\n",
      "Epoch   881 , batch Loss= 0.013442, Training Accuracy= 1.00000\n",
      "Epoch   882 , batch Loss= 0.082931, Training Accuracy= 0.96000\n",
      "Epoch   883 , batch Loss= 0.072848, Training Accuracy= 0.98000\n",
      "Epoch   884 , batch Loss= 0.003091, Training Accuracy= 1.00000\n",
      "Epoch   885 , batch Loss= 0.003558, Training Accuracy= 1.00000\n",
      "Epoch   886 , batch Loss= 0.045518, Training Accuracy= 0.98000\n",
      "Epoch   887 , batch Loss= 0.085231, Training Accuracy= 0.97000\n",
      "Epoch   888 , batch Loss= 0.053287, Training Accuracy= 0.98000\n",
      "Epoch   889 , batch Loss= 0.018751, Training Accuracy= 1.00000\n",
      "Epoch   890 , batch Loss= 0.004194, Training Accuracy= 1.00000\n",
      "Epoch   891 , batch Loss= 0.007973, Training Accuracy= 1.00000\n",
      "Epoch   892 , batch Loss= 0.019102, Training Accuracy= 1.00000\n",
      "Epoch   893 , batch Loss= 0.022525, Training Accuracy= 1.00000\n",
      "Epoch   894 , batch Loss= 0.015962, Training Accuracy= 1.00000\n",
      "Epoch   895 , batch Loss= 0.036772, Training Accuracy= 0.99000\n",
      "Epoch   896 , batch Loss= 0.006719, Training Accuracy= 1.00000\n",
      "Epoch   897 , batch Loss= 0.010719, Training Accuracy= 1.00000\n",
      "Epoch   898 , batch Loss= 0.010810, Training Accuracy= 1.00000\n",
      "Epoch   899 , batch Loss= 0.017422, Training Accuracy= 1.00000\n",
      "Epoch   900 , batch Loss= 0.018015, Training Accuracy= 0.99000\n",
      "Epoch   901 , batch Loss= 0.018162, Training Accuracy= 0.99000\n",
      "Epoch   902 , batch Loss= 0.130425, Training Accuracy= 0.97000\n",
      "Epoch   903 , batch Loss= 0.122933, Training Accuracy= 0.97000\n",
      "Epoch   904 , batch Loss= 0.044895, Training Accuracy= 0.99000\n",
      "Epoch   905 , batch Loss= 0.070297, Training Accuracy= 0.99000\n",
      "Epoch   906 , batch Loss= 0.018927, Training Accuracy= 0.99000\n",
      "Epoch   907 , batch Loss= 0.060023, Training Accuracy= 0.99000\n",
      "Epoch   908 , batch Loss= 0.034790, Training Accuracy= 0.99000\n",
      "Epoch   909 , batch Loss= 0.052644, Training Accuracy= 0.98000\n",
      "Epoch   910 , batch Loss= 0.035013, Training Accuracy= 0.98000\n",
      "Epoch   911 , batch Loss= 0.020733, Training Accuracy= 0.99000\n",
      "Epoch   912 , batch Loss= 0.018482, Training Accuracy= 1.00000\n",
      "Epoch   913 , batch Loss= 0.023828, Training Accuracy= 1.00000\n",
      "Epoch   914 , batch Loss= 0.080922, Training Accuracy= 0.97000\n",
      "Epoch   915 , batch Loss= 0.005859, Training Accuracy= 1.00000\n",
      "Epoch   916 , batch Loss= 0.020326, Training Accuracy= 0.99000\n",
      "Epoch   917 , batch Loss= 0.016827, Training Accuracy= 1.00000\n",
      "Epoch   918 , batch Loss= 0.032754, Training Accuracy= 0.98000\n",
      "Epoch   919 , batch Loss= 0.044082, Training Accuracy= 0.98000\n",
      "Epoch   920 , batch Loss= 0.015806, Training Accuracy= 1.00000\n",
      "Epoch   921 , batch Loss= 0.005405, Training Accuracy= 1.00000\n",
      "Epoch   922 , batch Loss= 0.014431, Training Accuracy= 1.00000\n",
      "Epoch   923 , batch Loss= 0.004800, Training Accuracy= 1.00000\n",
      "Epoch   924 , batch Loss= 0.023708, Training Accuracy= 0.99000\n",
      "Epoch   925 , batch Loss= 0.035318, Training Accuracy= 0.99000\n",
      "Epoch   926 , batch Loss= 0.024099, Training Accuracy= 0.99000\n",
      "Epoch   927 , batch Loss= 0.019490, Training Accuracy= 0.99000\n",
      "Epoch   928 , batch Loss= 0.034401, Training Accuracy= 0.99000\n",
      "Epoch   929 , batch Loss= 0.012704, Training Accuracy= 1.00000\n",
      "Epoch   930 , batch Loss= 0.015465, Training Accuracy= 0.99000\n",
      "Epoch   931 , batch Loss= 0.009066, Training Accuracy= 1.00000\n",
      "Epoch   932 , batch Loss= 0.005717, Training Accuracy= 1.00000\n",
      "Epoch   933 , batch Loss= 0.006195, Training Accuracy= 1.00000\n",
      "Epoch   934 , batch Loss= 0.032121, Training Accuracy= 0.99000\n",
      "Epoch   935 , batch Loss= 0.014962, Training Accuracy= 0.99000\n",
      "Epoch   936 , batch Loss= 0.024143, Training Accuracy= 0.99000\n",
      "Epoch   937 , batch Loss= 0.027551, Training Accuracy= 0.98000\n",
      "Epoch   938 , batch Loss= 0.008533, Training Accuracy= 1.00000\n",
      "Epoch   939 , batch Loss= 0.087712, Training Accuracy= 0.97000\n",
      "Epoch   940 , batch Loss= 0.007165, Training Accuracy= 1.00000\n",
      "Epoch   941 , batch Loss= 0.051088, Training Accuracy= 0.99000\n",
      "Epoch   942 , batch Loss= 0.047077, Training Accuracy= 0.99000\n",
      "Epoch   943 , batch Loss= 0.016122, Training Accuracy= 1.00000\n",
      "Epoch   944 , batch Loss= 0.010541, Training Accuracy= 1.00000\n",
      "Epoch   945 , batch Loss= 0.019268, Training Accuracy= 1.00000\n",
      "Epoch   946 , batch Loss= 0.057835, Training Accuracy= 0.99000\n",
      "Epoch   947 , batch Loss= 0.012676, Training Accuracy= 0.99000\n",
      "Epoch   948 , batch Loss= 0.008364, Training Accuracy= 1.00000\n",
      "Epoch   949 , batch Loss= 0.017662, Training Accuracy= 1.00000\n",
      "Epoch   950 , batch Loss= 0.005773, Training Accuracy= 1.00000\n",
      "Epoch   951 , batch Loss= 0.004076, Training Accuracy= 1.00000\n",
      "Epoch   952 , batch Loss= 0.022472, Training Accuracy= 0.99000\n",
      "Epoch   953 , batch Loss= 0.091473, Training Accuracy= 0.97000\n",
      "Epoch   954 , batch Loss= 0.023478, Training Accuracy= 0.99000\n",
      "Epoch   955 , batch Loss= 0.009067, Training Accuracy= 1.00000\n",
      "Epoch   956 , batch Loss= 0.021104, Training Accuracy= 0.99000\n",
      "Epoch   957 , batch Loss= 0.008076, Training Accuracy= 1.00000\n",
      "Epoch   958 , batch Loss= 0.035194, Training Accuracy= 0.99000\n",
      "Epoch   959 , batch Loss= 0.083842, Training Accuracy= 0.97000\n",
      "Epoch   960 , batch Loss= 0.009079, Training Accuracy= 1.00000\n",
      "Epoch   961 , batch Loss= 0.008938, Training Accuracy= 1.00000\n",
      "Epoch   962 , batch Loss= 0.006604, Training Accuracy= 1.00000\n",
      "Epoch   963 , batch Loss= 0.064519, Training Accuracy= 0.99000\n",
      "Epoch   964 , batch Loss= 0.046175, Training Accuracy= 0.98000\n",
      "Epoch   965 , batch Loss= 0.029954, Training Accuracy= 0.99000\n",
      "Epoch   966 , batch Loss= 0.033354, Training Accuracy= 0.99000\n",
      "Epoch   967 , batch Loss= 0.017229, Training Accuracy= 1.00000\n",
      "Epoch   968 , batch Loss= 0.029041, Training Accuracy= 0.99000\n",
      "Epoch   969 , batch Loss= 0.018710, Training Accuracy= 0.99000\n",
      "Epoch   970 , batch Loss= 0.024191, Training Accuracy= 0.99000\n",
      "Epoch   971 , batch Loss= 0.028422, Training Accuracy= 0.99000\n",
      "Epoch   972 , batch Loss= 0.092012, Training Accuracy= 0.98000\n",
      "Epoch   973 , batch Loss= 0.012727, Training Accuracy= 1.00000\n",
      "Epoch   974 , batch Loss= 0.053109, Training Accuracy= 0.99000\n",
      "Epoch   975 , batch Loss= 0.050057, Training Accuracy= 0.97000\n",
      "Epoch   976 , batch Loss= 0.028307, Training Accuracy= 0.99000\n",
      "Epoch   977 , batch Loss= 0.029439, Training Accuracy= 1.00000\n",
      "Epoch   978 , batch Loss= 0.004822, Training Accuracy= 1.00000\n",
      "Epoch   979 , batch Loss= 0.054818, Training Accuracy= 0.98000\n",
      "Epoch   980 , batch Loss= 0.013253, Training Accuracy= 1.00000\n",
      "Epoch   981 , batch Loss= 0.042960, Training Accuracy= 0.98000\n",
      "Epoch   982 , batch Loss= 0.012758, Training Accuracy= 1.00000\n",
      "Epoch   983 , batch Loss= 0.045302, Training Accuracy= 0.99000\n",
      "Epoch   984 , batch Loss= 0.032028, Training Accuracy= 0.98000\n",
      "Epoch   985 , batch Loss= 0.007745, Training Accuracy= 1.00000\n",
      "Epoch   986 , batch Loss= 0.015693, Training Accuracy= 0.99000\n",
      "Epoch   987 , batch Loss= 0.002715, Training Accuracy= 1.00000\n",
      "Epoch   988 , batch Loss= 0.014708, Training Accuracy= 1.00000\n",
      "Epoch   989 , batch Loss= 0.183382, Training Accuracy= 0.97000\n",
      "Epoch   990 , batch Loss= 0.010057, Training Accuracy= 1.00000\n",
      "Epoch   991 , batch Loss= 0.025222, Training Accuracy= 1.00000\n",
      "Epoch   992 , batch Loss= 0.008270, Training Accuracy= 1.00000\n",
      "Epoch   993 , batch Loss= 0.027413, Training Accuracy= 0.99000\n",
      "Epoch   994 , batch Loss= 0.079392, Training Accuracy= 0.98000\n",
      "Epoch   995 , batch Loss= 0.059875, Training Accuracy= 0.99000\n",
      "Epoch   996 , batch Loss= 0.032334, Training Accuracy= 0.99000\n",
      "Epoch   997 , batch Loss= 0.011797, Training Accuracy= 1.00000\n",
      "Epoch   998 , batch Loss= 0.003791, Training Accuracy= 1.00000\n",
      "Epoch   999 , batch Loss= 0.012154, Training Accuracy= 1.00000\n",
      "Epoch   1000 , batch Loss= 0.017218, Training Accuracy= 0.99000\n",
      "Epoch   1001 , batch Loss= 0.008359, Training Accuracy= 1.00000\n",
      "Epoch   1002 , batch Loss= 0.007672, Training Accuracy= 1.00000\n",
      "Epoch   1003 , batch Loss= 0.016260, Training Accuracy= 1.00000\n",
      "Epoch   1004 , batch Loss= 0.032533, Training Accuracy= 0.99000\n",
      "Epoch   1005 , batch Loss= 0.053613, Training Accuracy= 0.98000\n",
      "Epoch   1006 , batch Loss= 0.050535, Training Accuracy= 0.98000\n",
      "Epoch   1007 , batch Loss= 0.035556, Training Accuracy= 0.98000\n",
      "Epoch   1008 , batch Loss= 0.059061, Training Accuracy= 0.99000\n",
      "Epoch   1009 , batch Loss= 0.072101, Training Accuracy= 0.97000\n",
      "Epoch   1010 , batch Loss= 0.012698, Training Accuracy= 1.00000\n",
      "Epoch   1011 , batch Loss= 0.008526, Training Accuracy= 1.00000\n",
      "Epoch   1012 , batch Loss= 0.091159, Training Accuracy= 0.98000\n",
      "Epoch   1013 , batch Loss= 0.001263, Training Accuracy= 1.00000\n",
      "Epoch   1014 , batch Loss= 0.016085, Training Accuracy= 1.00000\n",
      "Epoch   1015 , batch Loss= 0.042167, Training Accuracy= 0.99000\n",
      "Epoch   1016 , batch Loss= 0.002871, Training Accuracy= 1.00000\n",
      "Epoch   1017 , batch Loss= 0.005773, Training Accuracy= 1.00000\n",
      "Epoch   1018 , batch Loss= 0.007358, Training Accuracy= 1.00000\n",
      "Epoch   1019 , batch Loss= 0.005444, Training Accuracy= 1.00000\n",
      "Epoch   1020 , batch Loss= 0.018039, Training Accuracy= 1.00000\n",
      "Epoch   1021 , batch Loss= 0.027706, Training Accuracy= 0.98000\n",
      "Epoch   1022 , batch Loss= 0.029075, Training Accuracy= 0.99000\n",
      "Epoch   1023 , batch Loss= 0.063013, Training Accuracy= 0.98000\n",
      "Epoch   1024 , batch Loss= 0.026954, Training Accuracy= 0.99000\n",
      "Epoch   1025 , batch Loss= 0.004051, Training Accuracy= 1.00000\n",
      "Epoch   1026 , batch Loss= 0.052311, Training Accuracy= 0.98000\n",
      "Epoch   1027 , batch Loss= 0.013206, Training Accuracy= 0.99000\n",
      "Epoch   1028 , batch Loss= 0.011470, Training Accuracy= 1.00000\n",
      "Epoch   1029 , batch Loss= 0.033062, Training Accuracy= 0.98000\n",
      "Epoch   1030 , batch Loss= 0.001878, Training Accuracy= 1.00000\n",
      "Epoch   1031 , batch Loss= 0.002996, Training Accuracy= 1.00000\n",
      "Epoch   1032 , batch Loss= 0.006628, Training Accuracy= 1.00000\n",
      "Epoch   1033 , batch Loss= 0.012674, Training Accuracy= 1.00000\n",
      "Epoch   1034 , batch Loss= 0.213836, Training Accuracy= 0.98000\n",
      "Epoch   1035 , batch Loss= 0.064115, Training Accuracy= 0.98000\n",
      "Epoch   1036 , batch Loss= 0.031341, Training Accuracy= 0.99000\n",
      "Epoch   1037 , batch Loss= 0.003495, Training Accuracy= 1.00000\n",
      "Epoch   1038 , batch Loss= 0.024723, Training Accuracy= 0.98000\n",
      "Epoch   1039 , batch Loss= 0.061671, Training Accuracy= 0.99000\n",
      "Epoch   1040 , batch Loss= 0.011638, Training Accuracy= 1.00000\n",
      "Epoch   1041 , batch Loss= 0.062427, Training Accuracy= 0.98000\n",
      "Epoch   1042 , batch Loss= 0.012320, Training Accuracy= 1.00000\n",
      "Epoch   1043 , batch Loss= 0.010720, Training Accuracy= 1.00000\n",
      "Epoch   1044 , batch Loss= 0.016454, Training Accuracy= 1.00000\n",
      "Epoch   1045 , batch Loss= 0.010398, Training Accuracy= 1.00000\n",
      "Epoch   1046 , batch Loss= 0.014530, Training Accuracy= 1.00000\n",
      "Epoch   1047 , batch Loss= 0.033628, Training Accuracy= 0.99000\n",
      "Epoch   1048 , batch Loss= 0.014631, Training Accuracy= 1.00000\n",
      "Epoch   1049 , batch Loss= 0.011503, Training Accuracy= 1.00000\n",
      "Epoch   1050 , batch Loss= 0.012489, Training Accuracy= 1.00000\n",
      "Epoch   1051 , batch Loss= 0.006961, Training Accuracy= 1.00000\n",
      "Epoch   1052 , batch Loss= 0.010481, Training Accuracy= 0.99000\n",
      "Epoch   1053 , batch Loss= 0.024319, Training Accuracy= 0.99000\n",
      "Epoch   1054 , batch Loss= 0.081147, Training Accuracy= 0.98000\n",
      "Epoch   1055 , batch Loss= 0.017137, Training Accuracy= 1.00000\n",
      "Epoch   1056 , batch Loss= 0.009943, Training Accuracy= 1.00000\n",
      "Epoch   1057 , batch Loss= 0.021301, Training Accuracy= 0.99000\n",
      "Epoch   1058 , batch Loss= 0.005661, Training Accuracy= 1.00000\n",
      "Epoch   1059 , batch Loss= 0.017744, Training Accuracy= 0.99000\n",
      "Epoch   1060 , batch Loss= 0.011376, Training Accuracy= 1.00000\n",
      "Epoch   1061 , batch Loss= 0.010637, Training Accuracy= 1.00000\n",
      "Epoch   1062 , batch Loss= 0.004301, Training Accuracy= 1.00000\n",
      "Epoch   1063 , batch Loss= 0.034144, Training Accuracy= 0.99000\n",
      "Epoch   1064 , batch Loss= 0.005804, Training Accuracy= 1.00000\n",
      "Epoch   1065 , batch Loss= 0.011857, Training Accuracy= 1.00000\n",
      "Epoch   1066 , batch Loss= 0.046236, Training Accuracy= 0.98000\n",
      "Epoch   1067 , batch Loss= 0.029887, Training Accuracy= 0.98000\n",
      "Epoch   1068 , batch Loss= 0.010154, Training Accuracy= 1.00000\n",
      "Epoch   1069 , batch Loss= 0.046241, Training Accuracy= 0.99000\n",
      "Epoch   1070 , batch Loss= 0.005869, Training Accuracy= 1.00000\n",
      "Epoch   1071 , batch Loss= 0.019762, Training Accuracy= 0.99000\n",
      "Epoch   1072 , batch Loss= 0.037254, Training Accuracy= 0.99000\n",
      "Epoch   1073 , batch Loss= 0.012924, Training Accuracy= 1.00000\n",
      "Epoch   1074 , batch Loss= 0.040357, Training Accuracy= 0.99000\n",
      "Epoch   1075 , batch Loss= 0.014535, Training Accuracy= 0.99000\n",
      "Epoch   1076 , batch Loss= 0.019325, Training Accuracy= 0.99000\n",
      "Epoch   1077 , batch Loss= 0.022600, Training Accuracy= 0.99000\n",
      "Epoch   1078 , batch Loss= 0.034059, Training Accuracy= 0.99000\n",
      "Epoch   1079 , batch Loss= 0.022380, Training Accuracy= 1.00000\n",
      "Epoch   1080 , batch Loss= 0.022909, Training Accuracy= 0.98000\n",
      "Epoch   1081 , batch Loss= 0.072585, Training Accuracy= 0.96000\n",
      "Epoch   1082 , batch Loss= 0.009056, Training Accuracy= 1.00000\n",
      "Epoch   1083 , batch Loss= 0.000979, Training Accuracy= 1.00000\n",
      "Epoch   1084 , batch Loss= 0.009853, Training Accuracy= 1.00000\n",
      "Epoch   1085 , batch Loss= 0.072506, Training Accuracy= 0.98000\n",
      "Epoch   1086 , batch Loss= 0.029368, Training Accuracy= 0.99000\n",
      "Epoch   1087 , batch Loss= 0.022019, Training Accuracy= 0.99000\n",
      "Epoch   1088 , batch Loss= 0.008096, Training Accuracy= 1.00000\n",
      "Epoch   1089 , batch Loss= 0.011488, Training Accuracy= 1.00000\n",
      "Epoch   1090 , batch Loss= 0.006963, Training Accuracy= 1.00000\n",
      "Epoch   1091 , batch Loss= 0.007667, Training Accuracy= 1.00000\n",
      "Epoch   1092 , batch Loss= 0.005332, Training Accuracy= 1.00000\n",
      "Epoch   1093 , batch Loss= 0.027448, Training Accuracy= 0.98000\n",
      "Epoch   1094 , batch Loss= 0.037692, Training Accuracy= 0.98000\n",
      "Epoch   1095 , batch Loss= 0.015535, Training Accuracy= 0.99000\n",
      "Epoch   1096 , batch Loss= 0.033046, Training Accuracy= 0.98000\n",
      "Epoch   1097 , batch Loss= 0.045696, Training Accuracy= 0.98000\n",
      "Epoch   1098 , batch Loss= 0.061480, Training Accuracy= 0.97000\n",
      "Epoch   1099 , batch Loss= 0.022376, Training Accuracy= 0.99000\n",
      "Epoch   1100 , batch Loss= 0.053670, Training Accuracy= 0.99000\n",
      "Epoch   1101 , batch Loss= 0.038363, Training Accuracy= 0.99000\n",
      "Epoch   1102 , batch Loss= 0.008784, Training Accuracy= 1.00000\n",
      "Epoch   1103 , batch Loss= 0.007006, Training Accuracy= 1.00000\n",
      "Epoch   1104 , batch Loss= 0.014080, Training Accuracy= 1.00000\n",
      "Epoch   1105 , batch Loss= 0.006200, Training Accuracy= 1.00000\n",
      "Epoch   1106 , batch Loss= 0.008591, Training Accuracy= 1.00000\n",
      "Epoch   1107 , batch Loss= 0.022253, Training Accuracy= 1.00000\n",
      "Epoch   1108 , batch Loss= 0.021866, Training Accuracy= 0.98000\n",
      "Epoch   1109 , batch Loss= 0.009476, Training Accuracy= 1.00000\n",
      "Epoch   1110 , batch Loss= 0.010997, Training Accuracy= 0.99000\n",
      "Epoch   1111 , batch Loss= 0.031472, Training Accuracy= 0.98000\n",
      "Epoch   1112 , batch Loss= 0.024489, Training Accuracy= 0.99000\n",
      "Epoch   1113 , batch Loss= 0.011199, Training Accuracy= 1.00000\n",
      "Epoch   1114 , batch Loss= 0.049089, Training Accuracy= 0.99000\n",
      "Epoch   1115 , batch Loss= 0.016995, Training Accuracy= 0.99000\n",
      "Epoch   1116 , batch Loss= 0.004076, Training Accuracy= 1.00000\n",
      "Epoch   1117 , batch Loss= 0.014892, Training Accuracy= 0.99000\n",
      "Epoch   1118 , batch Loss= 0.005386, Training Accuracy= 1.00000\n",
      "Epoch   1119 , batch Loss= 0.020858, Training Accuracy= 0.99000\n",
      "Epoch   1120 , batch Loss= 0.010064, Training Accuracy= 1.00000\n",
      "Epoch   1121 , batch Loss= 0.006201, Training Accuracy= 1.00000\n",
      "Epoch   1122 , batch Loss= 0.015779, Training Accuracy= 1.00000\n",
      "Epoch   1123 , batch Loss= 0.009802, Training Accuracy= 0.99000\n",
      "Epoch   1124 , batch Loss= 0.010881, Training Accuracy= 1.00000\n",
      "Epoch   1125 , batch Loss= 0.007324, Training Accuracy= 1.00000\n",
      "Epoch   1126 , batch Loss= 0.028533, Training Accuracy= 0.99000\n",
      "Epoch   1127 , batch Loss= 0.012059, Training Accuracy= 1.00000\n",
      "Epoch   1128 , batch Loss= 0.003910, Training Accuracy= 1.00000\n",
      "Epoch   1129 , batch Loss= 0.019231, Training Accuracy= 0.99000\n",
      "Epoch   1130 , batch Loss= 0.015839, Training Accuracy= 0.99000\n",
      "Epoch   1131 , batch Loss= 0.005864, Training Accuracy= 1.00000\n",
      "Epoch   1132 , batch Loss= 0.052139, Training Accuracy= 0.98000\n",
      "Epoch   1133 , batch Loss= 0.028353, Training Accuracy= 0.99000\n",
      "Epoch   1134 , batch Loss= 0.003305, Training Accuracy= 1.00000\n",
      "Epoch   1135 , batch Loss= 0.006206, Training Accuracy= 1.00000\n",
      "Epoch   1136 , batch Loss= 0.006884, Training Accuracy= 1.00000\n",
      "Epoch   1137 , batch Loss= 0.034352, Training Accuracy= 0.99000\n",
      "Epoch   1138 , batch Loss= 0.005813, Training Accuracy= 1.00000\n",
      "Epoch   1139 , batch Loss= 0.002682, Training Accuracy= 1.00000\n",
      "Epoch   1140 , batch Loss= 0.003901, Training Accuracy= 1.00000\n",
      "Epoch   1141 , batch Loss= 0.010883, Training Accuracy= 1.00000\n",
      "Epoch   1142 , batch Loss= 0.009889, Training Accuracy= 1.00000\n",
      "Epoch   1143 , batch Loss= 0.014010, Training Accuracy= 0.99000\n",
      "Epoch   1144 , batch Loss= 0.004380, Training Accuracy= 1.00000\n",
      "Epoch   1145 , batch Loss= 0.001479, Training Accuracy= 1.00000\n",
      "Epoch   1146 , batch Loss= 0.000495, Training Accuracy= 1.00000\n",
      "Epoch   1147 , batch Loss= 0.004072, Training Accuracy= 1.00000\n",
      "Epoch   1148 , batch Loss= 0.017088, Training Accuracy= 0.99000\n",
      "Epoch   1149 , batch Loss= 0.007530, Training Accuracy= 1.00000\n",
      "Epoch   1150 , batch Loss= 0.011790, Training Accuracy= 1.00000\n",
      "Epoch   1151 , batch Loss= 0.005365, Training Accuracy= 1.00000\n",
      "Epoch   1152 , batch Loss= 0.021962, Training Accuracy= 0.99000\n",
      "Epoch   1153 , batch Loss= 0.006851, Training Accuracy= 1.00000\n",
      "Epoch   1154 , batch Loss= 0.012785, Training Accuracy= 1.00000\n",
      "Epoch   1155 , batch Loss= 0.006807, Training Accuracy= 1.00000\n",
      "Epoch   1156 , batch Loss= 0.002106, Training Accuracy= 1.00000\n",
      "Epoch   1157 , batch Loss= 0.001964, Training Accuracy= 1.00000\n",
      "Epoch   1158 , batch Loss= 0.026214, Training Accuracy= 0.99000\n",
      "Epoch   1159 , batch Loss= 0.005004, Training Accuracy= 1.00000\n",
      "Epoch   1160 , batch Loss= 0.009038, Training Accuracy= 1.00000\n",
      "Epoch   1161 , batch Loss= 0.017813, Training Accuracy= 1.00000\n",
      "Epoch   1162 , batch Loss= 0.002800, Training Accuracy= 1.00000\n",
      "Epoch   1163 , batch Loss= 0.005278, Training Accuracy= 1.00000\n",
      "Epoch   1164 , batch Loss= 0.008118, Training Accuracy= 1.00000\n",
      "Epoch   1165 , batch Loss= 0.048058, Training Accuracy= 0.99000\n",
      "Epoch   1166 , batch Loss= 0.002213, Training Accuracy= 1.00000\n",
      "Epoch   1167 , batch Loss= 0.002466, Training Accuracy= 1.00000\n",
      "Epoch   1168 , batch Loss= 0.005974, Training Accuracy= 1.00000\n",
      "Epoch   1169 , batch Loss= 0.009811, Training Accuracy= 1.00000\n",
      "Epoch   1170 , batch Loss= 0.014619, Training Accuracy= 1.00000\n",
      "Epoch   1171 , batch Loss= 0.001940, Training Accuracy= 1.00000\n",
      "Epoch   1172 , batch Loss= 0.002394, Training Accuracy= 1.00000\n",
      "Epoch   1173 , batch Loss= 0.009236, Training Accuracy= 0.99000\n",
      "Epoch   1174 , batch Loss= 0.010498, Training Accuracy= 1.00000\n",
      "Epoch   1175 , batch Loss= 0.026878, Training Accuracy= 0.99000\n",
      "Epoch   1176 , batch Loss= 0.011436, Training Accuracy= 0.99000\n",
      "Epoch   1177 , batch Loss= 0.008481, Training Accuracy= 1.00000\n",
      "Epoch   1178 , batch Loss= 0.005828, Training Accuracy= 1.00000\n",
      "Epoch   1179 , batch Loss= 0.008789, Training Accuracy= 1.00000\n",
      "Epoch   1180 , batch Loss= 0.003319, Training Accuracy= 1.00000\n",
      "Epoch   1181 , batch Loss= 0.001147, Training Accuracy= 1.00000\n",
      "Epoch   1182 , batch Loss= 0.008297, Training Accuracy= 1.00000\n",
      "Epoch   1183 , batch Loss= 0.034862, Training Accuracy= 0.99000\n",
      "Epoch   1184 , batch Loss= 0.011046, Training Accuracy= 1.00000\n",
      "Epoch   1185 , batch Loss= 0.008990, Training Accuracy= 1.00000\n",
      "Epoch   1186 , batch Loss= 0.002036, Training Accuracy= 1.00000\n",
      "Epoch   1187 , batch Loss= 0.020838, Training Accuracy= 0.99000\n",
      "Epoch   1188 , batch Loss= 0.008495, Training Accuracy= 1.00000\n",
      "Epoch   1189 , batch Loss= 0.030061, Training Accuracy= 0.98000\n",
      "Epoch   1190 , batch Loss= 0.003147, Training Accuracy= 1.00000\n",
      "Epoch   1191 , batch Loss= 0.017132, Training Accuracy= 1.00000\n",
      "Epoch   1192 , batch Loss= 0.007603, Training Accuracy= 1.00000\n",
      "Epoch   1193 , batch Loss= 0.008365, Training Accuracy= 1.00000\n",
      "Epoch   1194 , batch Loss= 0.035540, Training Accuracy= 0.99000\n",
      "Epoch   1195 , batch Loss= 0.003157, Training Accuracy= 1.00000\n",
      "Epoch   1196 , batch Loss= 0.011909, Training Accuracy= 0.99000\n",
      "Epoch   1197 , batch Loss= 0.007352, Training Accuracy= 1.00000\n",
      "Epoch   1198 , batch Loss= 0.016384, Training Accuracy= 0.99000\n",
      "Epoch   1199 , batch Loss= 0.021621, Training Accuracy= 0.99000\n",
      "Epoch   1200 , batch Loss= 0.002824, Training Accuracy= 1.00000\n",
      "Epoch   1201 , batch Loss= 0.029130, Training Accuracy= 0.99000\n",
      "Epoch   1202 , batch Loss= 0.027928, Training Accuracy= 0.98000\n",
      "Epoch   1203 , batch Loss= 0.022615, Training Accuracy= 0.98000\n",
      "Epoch   1204 , batch Loss= 0.006984, Training Accuracy= 1.00000\n",
      "Epoch   1205 , batch Loss= 0.003881, Training Accuracy= 1.00000\n",
      "Epoch   1206 , batch Loss= 0.036706, Training Accuracy= 0.99000\n",
      "Epoch   1207 , batch Loss= 0.003474, Training Accuracy= 1.00000\n",
      "Epoch   1208 , batch Loss= 0.007241, Training Accuracy= 1.00000\n",
      "Epoch   1209 , batch Loss= 0.019031, Training Accuracy= 0.99000\n",
      "Epoch   1210 , batch Loss= 0.019233, Training Accuracy= 0.99000\n",
      "Epoch   1211 , batch Loss= 0.078568, Training Accuracy= 0.98000\n",
      "Epoch   1212 , batch Loss= 0.002392, Training Accuracy= 1.00000\n",
      "Epoch   1213 , batch Loss= 0.018314, Training Accuracy= 0.99000\n",
      "Epoch   1214 , batch Loss= 0.081583, Training Accuracy= 0.99000\n",
      "Epoch   1215 , batch Loss= 0.003115, Training Accuracy= 1.00000\n",
      "Epoch   1216 , batch Loss= 0.019953, Training Accuracy= 0.99000\n",
      "Epoch   1217 , batch Loss= 0.004232, Training Accuracy= 1.00000\n",
      "Epoch   1218 , batch Loss= 0.013058, Training Accuracy= 0.99000\n",
      "Epoch   1219 , batch Loss= 0.021520, Training Accuracy= 0.99000\n",
      "Epoch   1220 , batch Loss= 0.020776, Training Accuracy= 0.99000\n",
      "Epoch   1221 , batch Loss= 0.012932, Training Accuracy= 1.00000\n",
      "Epoch   1222 , batch Loss= 0.039704, Training Accuracy= 0.99000\n",
      "Epoch   1223 , batch Loss= 0.021916, Training Accuracy= 0.99000\n",
      "Epoch   1224 , batch Loss= 0.030913, Training Accuracy= 0.99000\n",
      "Epoch   1225 , batch Loss= 0.005266, Training Accuracy= 1.00000\n",
      "Epoch   1226 , batch Loss= 0.005265, Training Accuracy= 1.00000\n",
      "Epoch   1227 , batch Loss= 0.006846, Training Accuracy= 1.00000\n",
      "Epoch   1228 , batch Loss= 0.048184, Training Accuracy= 0.99000\n",
      "Epoch   1229 , batch Loss= 0.040622, Training Accuracy= 0.99000\n",
      "Epoch   1230 , batch Loss= 0.002989, Training Accuracy= 1.00000\n",
      "Epoch   1231 , batch Loss= 0.009129, Training Accuracy= 1.00000\n",
      "Epoch   1232 , batch Loss= 0.009341, Training Accuracy= 1.00000\n",
      "Epoch   1233 , batch Loss= 0.006552, Training Accuracy= 1.00000\n",
      "Epoch   1234 , batch Loss= 0.008435, Training Accuracy= 1.00000\n",
      "Epoch   1235 , batch Loss= 0.007115, Training Accuracy= 1.00000\n",
      "Epoch   1236 , batch Loss= 0.074924, Training Accuracy= 0.99000\n",
      "Epoch   1237 , batch Loss= 0.010004, Training Accuracy= 1.00000\n",
      "Epoch   1238 , batch Loss= 0.008011, Training Accuracy= 1.00000\n",
      "Epoch   1239 , batch Loss= 0.005588, Training Accuracy= 1.00000\n",
      "Epoch   1240 , batch Loss= 0.007192, Training Accuracy= 1.00000\n",
      "Epoch   1241 , batch Loss= 0.003255, Training Accuracy= 1.00000\n",
      "Epoch   1242 , batch Loss= 0.002973, Training Accuracy= 1.00000\n",
      "Epoch   1243 , batch Loss= 0.013660, Training Accuracy= 1.00000\n",
      "Epoch   1244 , batch Loss= 0.004841, Training Accuracy= 1.00000\n",
      "Epoch   1245 , batch Loss= 0.005154, Training Accuracy= 1.00000\n",
      "Epoch   1246 , batch Loss= 0.063458, Training Accuracy= 0.99000\n",
      "Epoch   1247 , batch Loss= 0.021583, Training Accuracy= 0.99000\n",
      "Epoch   1248 , batch Loss= 0.021315, Training Accuracy= 0.99000\n",
      "Epoch   1249 , batch Loss= 0.010631, Training Accuracy= 0.99000\n",
      "Epoch   1250 , batch Loss= 0.006655, Training Accuracy= 1.00000\n",
      "Epoch   1251 , batch Loss= 0.006169, Training Accuracy= 1.00000\n",
      "Epoch   1252 , batch Loss= 0.036728, Training Accuracy= 0.99000\n",
      "Epoch   1253 , batch Loss= 0.009927, Training Accuracy= 1.00000\n",
      "Epoch   1254 , batch Loss= 0.012238, Training Accuracy= 0.99000\n",
      "Epoch   1255 , batch Loss= 0.010122, Training Accuracy= 1.00000\n",
      "Epoch   1256 , batch Loss= 0.023802, Training Accuracy= 0.99000\n",
      "Epoch   1257 , batch Loss= 0.034648, Training Accuracy= 0.98000\n",
      "Epoch   1258 , batch Loss= 0.011006, Training Accuracy= 0.99000\n",
      "Epoch   1259 , batch Loss= 0.010597, Training Accuracy= 1.00000\n",
      "Epoch   1260 , batch Loss= 0.003685, Training Accuracy= 1.00000\n",
      "Epoch   1261 , batch Loss= 0.010400, Training Accuracy= 1.00000\n",
      "Epoch   1262 , batch Loss= 0.008751, Training Accuracy= 1.00000\n",
      "Epoch   1263 , batch Loss= 0.003423, Training Accuracy= 1.00000\n",
      "Epoch   1264 , batch Loss= 0.003028, Training Accuracy= 1.00000\n",
      "Epoch   1265 , batch Loss= 0.022619, Training Accuracy= 0.99000\n",
      "Epoch   1266 , batch Loss= 0.004635, Training Accuracy= 1.00000\n",
      "Epoch   1267 , batch Loss= 0.001110, Training Accuracy= 1.00000\n",
      "Epoch   1268 , batch Loss= 0.001668, Training Accuracy= 1.00000\n",
      "Epoch   1269 , batch Loss= 0.005164, Training Accuracy= 1.00000\n",
      "Epoch   1270 , batch Loss= 0.022498, Training Accuracy= 0.99000\n",
      "Epoch   1271 , batch Loss= 0.010279, Training Accuracy= 0.99000\n",
      "Epoch   1272 , batch Loss= 0.011634, Training Accuracy= 1.00000\n",
      "Epoch   1273 , batch Loss= 0.023196, Training Accuracy= 0.99000\n",
      "Epoch   1274 , batch Loss= 0.014430, Training Accuracy= 1.00000\n",
      "Epoch   1275 , batch Loss= 0.004339, Training Accuracy= 1.00000\n",
      "Epoch   1276 , batch Loss= 0.001264, Training Accuracy= 1.00000\n",
      "Epoch   1277 , batch Loss= 0.044469, Training Accuracy= 0.99000\n",
      "Epoch   1278 , batch Loss= 0.001525, Training Accuracy= 1.00000\n",
      "Epoch   1279 , batch Loss= 0.019871, Training Accuracy= 0.99000\n",
      "Epoch   1280 , batch Loss= 0.038527, Training Accuracy= 0.99000\n",
      "Epoch   1281 , batch Loss= 0.017308, Training Accuracy= 0.99000\n",
      "Epoch   1282 , batch Loss= 0.035452, Training Accuracy= 0.99000\n",
      "Epoch   1283 , batch Loss= 0.010312, Training Accuracy= 1.00000\n",
      "Epoch   1284 , batch Loss= 0.022652, Training Accuracy= 0.99000\n",
      "Epoch   1285 , batch Loss= 0.004799, Training Accuracy= 1.00000\n",
      "Epoch   1286 , batch Loss= 0.003607, Training Accuracy= 1.00000\n",
      "Epoch   1287 , batch Loss= 0.004256, Training Accuracy= 1.00000\n",
      "Epoch   1288 , batch Loss= 0.013329, Training Accuracy= 1.00000\n",
      "Epoch   1289 , batch Loss= 0.009089, Training Accuracy= 1.00000\n",
      "Epoch   1290 , batch Loss= 0.004549, Training Accuracy= 1.00000\n",
      "Epoch   1291 , batch Loss= 0.078965, Training Accuracy= 0.99000\n",
      "Epoch   1292 , batch Loss= 0.011756, Training Accuracy= 0.99000\n",
      "Epoch   1293 , batch Loss= 0.016128, Training Accuracy= 1.00000\n",
      "Epoch   1294 , batch Loss= 0.014489, Training Accuracy= 0.99000\n",
      "Epoch   1295 , batch Loss= 0.121944, Training Accuracy= 0.96000\n",
      "Epoch   1296 , batch Loss= 0.025119, Training Accuracy= 0.99000\n",
      "Epoch   1297 , batch Loss= 0.010656, Training Accuracy= 1.00000\n",
      "Epoch   1298 , batch Loss= 0.007064, Training Accuracy= 1.00000\n",
      "Epoch   1299 , batch Loss= 0.012747, Training Accuracy= 1.00000\n",
      "Epoch   1300 , batch Loss= 0.011359, Training Accuracy= 1.00000\n",
      "Epoch   1301 , batch Loss= 0.008517, Training Accuracy= 1.00000\n",
      "Epoch   1302 , batch Loss= 0.017425, Training Accuracy= 0.99000\n",
      "Epoch   1303 , batch Loss= 0.017985, Training Accuracy= 1.00000\n",
      "Epoch   1304 , batch Loss= 0.008333, Training Accuracy= 1.00000\n",
      "Epoch   1305 , batch Loss= 0.032069, Training Accuracy= 0.99000\n",
      "Epoch   1306 , batch Loss= 0.001614, Training Accuracy= 1.00000\n",
      "Epoch   1307 , batch Loss= 0.011261, Training Accuracy= 1.00000\n",
      "Epoch   1308 , batch Loss= 0.006989, Training Accuracy= 1.00000\n",
      "Epoch   1309 , batch Loss= 0.043153, Training Accuracy= 0.99000\n",
      "Epoch   1310 , batch Loss= 0.032735, Training Accuracy= 0.98000\n",
      "Epoch   1311 , batch Loss= 0.069927, Training Accuracy= 0.99000\n",
      "Epoch   1312 , batch Loss= 0.002249, Training Accuracy= 1.00000\n",
      "Epoch   1313 , batch Loss= 0.011972, Training Accuracy= 1.00000\n",
      "Epoch   1314 , batch Loss= 0.012894, Training Accuracy= 1.00000\n",
      "Epoch   1315 , batch Loss= 0.012516, Training Accuracy= 0.99000\n",
      "Epoch   1316 , batch Loss= 0.003948, Training Accuracy= 1.00000\n",
      "Epoch   1317 , batch Loss= 0.004983, Training Accuracy= 1.00000\n",
      "Epoch   1318 , batch Loss= 0.002927, Training Accuracy= 1.00000\n",
      "Epoch   1319 , batch Loss= 0.002176, Training Accuracy= 1.00000\n",
      "Epoch   1320 , batch Loss= 0.005842, Training Accuracy= 1.00000\n",
      "Epoch   1321 , batch Loss= 0.010596, Training Accuracy= 1.00000\n",
      "Epoch   1322 , batch Loss= 0.006154, Training Accuracy= 1.00000\n",
      "Epoch   1323 , batch Loss= 0.003372, Training Accuracy= 1.00000\n",
      "Epoch   1324 , batch Loss= 0.009937, Training Accuracy= 1.00000\n",
      "Epoch   1325 , batch Loss= 0.001070, Training Accuracy= 1.00000\n",
      "Epoch   1326 , batch Loss= 0.013035, Training Accuracy= 1.00000\n",
      "Epoch   1327 , batch Loss= 0.029939, Training Accuracy= 0.99000\n",
      "Epoch   1328 , batch Loss= 0.012737, Training Accuracy= 1.00000\n",
      "Epoch   1329 , batch Loss= 0.002810, Training Accuracy= 1.00000\n",
      "Epoch   1330 , batch Loss= 0.005437, Training Accuracy= 1.00000\n",
      "Epoch   1331 , batch Loss= 0.023730, Training Accuracy= 0.99000\n",
      "Epoch   1332 , batch Loss= 0.050935, Training Accuracy= 0.99000\n",
      "Epoch   1333 , batch Loss= 0.007664, Training Accuracy= 1.00000\n",
      "Epoch   1334 , batch Loss= 0.005161, Training Accuracy= 1.00000\n",
      "Epoch   1335 , batch Loss= 0.002145, Training Accuracy= 1.00000\n",
      "Epoch   1336 , batch Loss= 0.002909, Training Accuracy= 1.00000\n",
      "Epoch   1337 , batch Loss= 0.012172, Training Accuracy= 1.00000\n",
      "Epoch   1338 , batch Loss= 0.011067, Training Accuracy= 1.00000\n",
      "Epoch   1339 , batch Loss= 0.011069, Training Accuracy= 1.00000\n",
      "Epoch   1340 , batch Loss= 0.002436, Training Accuracy= 1.00000\n",
      "Epoch   1341 , batch Loss= 0.019075, Training Accuracy= 1.00000\n",
      "Epoch   1342 , batch Loss= 0.003378, Training Accuracy= 1.00000\n",
      "Epoch   1343 , batch Loss= 0.026048, Training Accuracy= 0.98000\n",
      "Epoch   1344 , batch Loss= 0.002982, Training Accuracy= 1.00000\n",
      "Epoch   1345 , batch Loss= 0.005647, Training Accuracy= 1.00000\n",
      "Epoch   1346 , batch Loss= 0.014068, Training Accuracy= 1.00000\n",
      "Epoch   1347 , batch Loss= 0.002510, Training Accuracy= 1.00000\n",
      "Epoch   1348 , batch Loss= 0.006820, Training Accuracy= 1.00000\n",
      "Epoch   1349 , batch Loss= 0.008789, Training Accuracy= 0.99000\n",
      "Epoch   1350 , batch Loss= 0.010729, Training Accuracy= 1.00000\n",
      "Epoch   1351 , batch Loss= 0.018240, Training Accuracy= 0.99000\n",
      "Epoch   1352 , batch Loss= 0.006009, Training Accuracy= 1.00000\n",
      "Epoch   1353 , batch Loss= 0.010736, Training Accuracy= 1.00000\n",
      "Epoch   1354 , batch Loss= 0.020876, Training Accuracy= 0.99000\n",
      "Epoch   1355 , batch Loss= 0.008613, Training Accuracy= 1.00000\n",
      "Epoch   1356 , batch Loss= 0.008138, Training Accuracy= 1.00000\n",
      "Epoch   1357 , batch Loss= 0.006364, Training Accuracy= 1.00000\n",
      "Epoch   1358 , batch Loss= 0.019876, Training Accuracy= 0.99000\n",
      "Epoch   1359 , batch Loss= 0.006904, Training Accuracy= 1.00000\n",
      "Epoch   1360 , batch Loss= 0.009094, Training Accuracy= 1.00000\n",
      "Epoch   1361 , batch Loss= 0.003985, Training Accuracy= 1.00000\n",
      "Epoch   1362 , batch Loss= 0.009946, Training Accuracy= 1.00000\n",
      "Epoch   1363 , batch Loss= 0.005223, Training Accuracy= 1.00000\n",
      "Epoch   1364 , batch Loss= 0.020722, Training Accuracy= 0.99000\n",
      "Epoch   1365 , batch Loss= 0.018524, Training Accuracy= 0.99000\n",
      "Epoch   1366 , batch Loss= 0.004043, Training Accuracy= 1.00000\n",
      "Epoch   1367 , batch Loss= 0.010439, Training Accuracy= 1.00000\n",
      "Epoch   1368 , batch Loss= 0.003044, Training Accuracy= 1.00000\n",
      "Epoch   1369 , batch Loss= 0.008688, Training Accuracy= 1.00000\n",
      "Epoch   1370 , batch Loss= 0.008299, Training Accuracy= 1.00000\n",
      "Epoch   1371 , batch Loss= 0.005453, Training Accuracy= 1.00000\n",
      "Epoch   1372 , batch Loss= 0.005738, Training Accuracy= 1.00000\n",
      "Epoch   1373 , batch Loss= 0.003445, Training Accuracy= 1.00000\n",
      "Epoch   1374 , batch Loss= 0.008868, Training Accuracy= 1.00000\n",
      "Epoch   1375 , batch Loss= 0.001501, Training Accuracy= 1.00000\n",
      "Epoch   1376 , batch Loss= 0.002933, Training Accuracy= 1.00000\n",
      "Epoch   1377 , batch Loss= 0.004288, Training Accuracy= 1.00000\n",
      "Epoch   1378 , batch Loss= 0.001454, Training Accuracy= 1.00000\n",
      "Epoch   1379 , batch Loss= 0.014759, Training Accuracy= 0.99000\n",
      "Epoch   1380 , batch Loss= 0.015305, Training Accuracy= 0.99000\n",
      "Epoch   1381 , batch Loss= 0.014850, Training Accuracy= 0.99000\n",
      "Epoch   1382 , batch Loss= 0.012014, Training Accuracy= 1.00000\n",
      "Epoch   1383 , batch Loss= 0.000981, Training Accuracy= 1.00000\n",
      "Epoch   1384 , batch Loss= 0.001845, Training Accuracy= 1.00000\n",
      "Epoch   1385 , batch Loss= 0.010043, Training Accuracy= 1.00000\n",
      "Epoch   1386 , batch Loss= 0.003475, Training Accuracy= 1.00000\n",
      "Epoch   1387 , batch Loss= 0.006969, Training Accuracy= 1.00000\n",
      "Epoch   1388 , batch Loss= 0.004080, Training Accuracy= 1.00000\n",
      "Epoch   1389 , batch Loss= 0.011395, Training Accuracy= 0.99000\n",
      "Epoch   1390 , batch Loss= 0.004679, Training Accuracy= 1.00000\n",
      "Epoch   1391 , batch Loss= 0.003002, Training Accuracy= 1.00000\n",
      "Epoch   1392 , batch Loss= 0.038542, Training Accuracy= 0.99000\n",
      "Epoch   1393 , batch Loss= 0.026104, Training Accuracy= 0.99000\n",
      "Epoch   1394 , batch Loss= 0.004131, Training Accuracy= 1.00000\n",
      "Epoch   1395 , batch Loss= 0.000582, Training Accuracy= 1.00000\n",
      "Epoch   1396 , batch Loss= 0.000788, Training Accuracy= 1.00000\n",
      "Epoch   1397 , batch Loss= 0.010809, Training Accuracy= 1.00000\n",
      "Epoch   1398 , batch Loss= 0.026935, Training Accuracy= 0.99000\n",
      "Epoch   1399 , batch Loss= 0.023043, Training Accuracy= 0.99000\n",
      "Epoch   1400 , batch Loss= 0.038854, Training Accuracy= 0.98000\n",
      "Epoch   1401 , batch Loss= 0.000862, Training Accuracy= 1.00000\n",
      "Epoch   1402 , batch Loss= 0.004401, Training Accuracy= 1.00000\n",
      "Epoch   1403 , batch Loss= 0.026895, Training Accuracy= 0.99000\n",
      "Epoch   1404 , batch Loss= 0.004693, Training Accuracy= 1.00000\n",
      "Epoch   1405 , batch Loss= 0.005854, Training Accuracy= 1.00000\n",
      "Epoch   1406 , batch Loss= 0.007004, Training Accuracy= 1.00000\n",
      "Epoch   1407 , batch Loss= 0.003688, Training Accuracy= 1.00000\n",
      "Epoch   1408 , batch Loss= 0.016767, Training Accuracy= 0.99000\n",
      "Epoch   1409 , batch Loss= 0.075678, Training Accuracy= 0.98000\n",
      "Epoch   1410 , batch Loss= 0.001925, Training Accuracy= 1.00000\n",
      "Epoch   1411 , batch Loss= 0.013152, Training Accuracy= 1.00000\n",
      "Epoch   1412 , batch Loss= 0.010108, Training Accuracy= 1.00000\n",
      "Epoch   1413 , batch Loss= 0.015271, Training Accuracy= 0.99000\n",
      "Epoch   1414 , batch Loss= 0.005713, Training Accuracy= 1.00000\n",
      "Epoch   1415 , batch Loss= 0.003065, Training Accuracy= 1.00000\n",
      "Epoch   1416 , batch Loss= 0.007617, Training Accuracy= 1.00000\n",
      "Epoch   1417 , batch Loss= 0.017637, Training Accuracy= 1.00000\n",
      "Epoch   1418 , batch Loss= 0.021251, Training Accuracy= 1.00000\n",
      "Epoch   1419 , batch Loss= 0.006317, Training Accuracy= 1.00000\n",
      "Epoch   1420 , batch Loss= 0.004322, Training Accuracy= 1.00000\n",
      "Epoch   1421 , batch Loss= 0.021196, Training Accuracy= 0.98000\n",
      "Epoch   1422 , batch Loss= 0.020875, Training Accuracy= 0.98000\n",
      "Epoch   1423 , batch Loss= 0.002589, Training Accuracy= 1.00000\n",
      "Epoch   1424 , batch Loss= 0.001641, Training Accuracy= 1.00000\n",
      "Epoch   1425 , batch Loss= 0.005296, Training Accuracy= 1.00000\n",
      "Epoch   1426 , batch Loss= 0.041708, Training Accuracy= 0.98000\n",
      "Epoch   1427 , batch Loss= 0.012742, Training Accuracy= 0.99000\n",
      "Epoch   1428 , batch Loss= 0.078677, Training Accuracy= 0.97000\n",
      "Epoch   1429 , batch Loss= 0.012985, Training Accuracy= 0.99000\n",
      "Epoch   1430 , batch Loss= 0.084570, Training Accuracy= 0.97000\n",
      "Epoch   1431 , batch Loss= 0.001877, Training Accuracy= 1.00000\n",
      "Epoch   1432 , batch Loss= 0.042915, Training Accuracy= 0.98000\n",
      "Epoch   1433 , batch Loss= 0.001287, Training Accuracy= 1.00000\n",
      "Epoch   1434 , batch Loss= 0.025330, Training Accuracy= 0.99000\n",
      "Epoch   1435 , batch Loss= 0.004246, Training Accuracy= 1.00000\n",
      "Epoch   1436 , batch Loss= 0.006261, Training Accuracy= 1.00000\n",
      "Epoch   1437 , batch Loss= 0.011578, Training Accuracy= 1.00000\n",
      "Epoch   1438 , batch Loss= 0.012751, Training Accuracy= 1.00000\n",
      "Epoch   1439 , batch Loss= 0.013029, Training Accuracy= 0.99000\n",
      "Epoch   1440 , batch Loss= 0.000534, Training Accuracy= 1.00000\n",
      "Epoch   1441 , batch Loss= 0.024037, Training Accuracy= 0.98000\n",
      "Epoch   1442 , batch Loss= 0.010508, Training Accuracy= 1.00000\n",
      "Epoch   1443 , batch Loss= 0.004039, Training Accuracy= 1.00000\n",
      "Epoch   1444 , batch Loss= 0.026809, Training Accuracy= 0.98000\n",
      "Epoch   1445 , batch Loss= 0.010782, Training Accuracy= 1.00000\n",
      "Epoch   1446 , batch Loss= 0.017691, Training Accuracy= 0.99000\n",
      "Epoch   1447 , batch Loss= 0.021593, Training Accuracy= 0.99000\n",
      "Epoch   1448 , batch Loss= 0.079458, Training Accuracy= 0.98000\n",
      "Epoch   1449 , batch Loss= 0.031368, Training Accuracy= 0.98000\n",
      "Epoch   1450 , batch Loss= 0.005607, Training Accuracy= 1.00000\n",
      "Epoch   1451 , batch Loss= 0.001233, Training Accuracy= 1.00000\n",
      "Epoch   1452 , batch Loss= 0.001988, Training Accuracy= 1.00000\n",
      "Epoch   1453 , batch Loss= 0.021714, Training Accuracy= 0.99000\n",
      "Epoch   1454 , batch Loss= 0.031479, Training Accuracy= 0.99000\n",
      "Epoch   1455 , batch Loss= 0.014518, Training Accuracy= 0.99000\n",
      "Epoch   1456 , batch Loss= 0.009848, Training Accuracy= 1.00000\n",
      "Epoch   1457 , batch Loss= 0.011263, Training Accuracy= 1.00000\n",
      "Epoch   1458 , batch Loss= 0.004527, Training Accuracy= 1.00000\n",
      "Epoch   1459 , batch Loss= 0.011093, Training Accuracy= 1.00000\n",
      "Epoch   1460 , batch Loss= 0.008860, Training Accuracy= 1.00000\n",
      "Epoch   1461 , batch Loss= 0.003547, Training Accuracy= 1.00000\n",
      "Epoch   1462 , batch Loss= 0.005129, Training Accuracy= 1.00000\n",
      "Epoch   1463 , batch Loss= 0.012392, Training Accuracy= 1.00000\n",
      "Epoch   1464 , batch Loss= 0.002383, Training Accuracy= 1.00000\n",
      "Epoch   1465 , batch Loss= 0.001124, Training Accuracy= 1.00000\n",
      "Epoch   1466 , batch Loss= 0.002562, Training Accuracy= 1.00000\n",
      "Epoch   1467 , batch Loss= 0.017179, Training Accuracy= 0.99000\n",
      "Epoch   1468 , batch Loss= 0.005321, Training Accuracy= 1.00000\n",
      "Epoch   1469 , batch Loss= 0.018601, Training Accuracy= 0.99000\n",
      "Epoch   1470 , batch Loss= 0.039207, Training Accuracy= 0.98000\n",
      "Epoch   1471 , batch Loss= 0.013329, Training Accuracy= 1.00000\n",
      "Epoch   1472 , batch Loss= 0.010915, Training Accuracy= 0.99000\n",
      "Epoch   1473 , batch Loss= 0.017301, Training Accuracy= 0.99000\n",
      "Epoch   1474 , batch Loss= 0.017344, Training Accuracy= 1.00000\n",
      "Epoch   1475 , batch Loss= 0.011796, Training Accuracy= 1.00000\n",
      "Epoch   1476 , batch Loss= 0.002871, Training Accuracy= 1.00000\n",
      "Epoch   1477 , batch Loss= 0.048493, Training Accuracy= 0.98000\n",
      "Epoch   1478 , batch Loss= 0.007104, Training Accuracy= 1.00000\n",
      "Epoch   1479 , batch Loss= 0.003538, Training Accuracy= 1.00000\n",
      "Epoch   1480 , batch Loss= 0.008908, Training Accuracy= 1.00000\n",
      "Epoch   1481 , batch Loss= 0.015176, Training Accuracy= 0.99000\n",
      "Epoch   1482 , batch Loss= 0.005063, Training Accuracy= 1.00000\n",
      "Epoch   1483 , batch Loss= 0.001189, Training Accuracy= 1.00000\n",
      "Epoch   1484 , batch Loss= 0.021354, Training Accuracy= 0.99000\n",
      "Epoch   1485 , batch Loss= 0.003075, Training Accuracy= 1.00000\n",
      "Epoch   1486 , batch Loss= 0.008456, Training Accuracy= 1.00000\n",
      "Epoch   1487 , batch Loss= 0.002510, Training Accuracy= 1.00000\n",
      "Epoch   1488 , batch Loss= 0.034338, Training Accuracy= 0.98000\n",
      "Epoch   1489 , batch Loss= 0.004394, Training Accuracy= 1.00000\n",
      "Epoch   1490 , batch Loss= 0.002101, Training Accuracy= 1.00000\n",
      "Epoch   1491 , batch Loss= 0.031266, Training Accuracy= 0.98000\n",
      "Epoch   1492 , batch Loss= 0.005919, Training Accuracy= 1.00000\n",
      "Epoch   1493 , batch Loss= 0.052618, Training Accuracy= 0.99000\n",
      "Epoch   1494 , batch Loss= 0.044377, Training Accuracy= 0.98000\n",
      "Epoch   1495 , batch Loss= 0.008925, Training Accuracy= 1.00000\n",
      "Epoch   1496 , batch Loss= 0.063275, Training Accuracy= 0.98000\n",
      "Epoch   1497 , batch Loss= 0.015016, Training Accuracy= 0.99000\n",
      "Epoch   1498 , batch Loss= 0.007563, Training Accuracy= 1.00000\n",
      "Epoch   1499 , batch Loss= 0.024050, Training Accuracy= 0.99000\n",
      "Epoch   1500 , batch Loss= 0.006368, Training Accuracy= 1.00000\n",
      "Epoch   1501 , batch Loss= 0.036225, Training Accuracy= 0.99000\n",
      "Epoch   1502 , batch Loss= 0.021252, Training Accuracy= 0.99000\n",
      "Epoch   1503 , batch Loss= 0.014744, Training Accuracy= 0.99000\n",
      "Epoch   1504 , batch Loss= 0.032246, Training Accuracy= 0.98000\n",
      "Epoch   1505 , batch Loss= 0.002926, Training Accuracy= 1.00000\n",
      "Epoch   1506 , batch Loss= 0.003070, Training Accuracy= 1.00000\n",
      "Epoch   1507 , batch Loss= 0.035973, Training Accuracy= 0.99000\n",
      "Epoch   1508 , batch Loss= 0.029009, Training Accuracy= 0.99000\n",
      "Epoch   1509 , batch Loss= 0.033609, Training Accuracy= 0.98000\n",
      "Epoch   1510 , batch Loss= 0.033673, Training Accuracy= 0.98000\n",
      "Epoch   1511 , batch Loss= 0.003495, Training Accuracy= 1.00000\n",
      "Epoch   1512 , batch Loss= 0.004096, Training Accuracy= 1.00000\n",
      "Epoch   1513 , batch Loss= 0.006239, Training Accuracy= 1.00000\n",
      "Epoch   1514 , batch Loss= 0.010530, Training Accuracy= 1.00000\n",
      "Epoch   1515 , batch Loss= 0.042259, Training Accuracy= 0.98000\n",
      "Epoch   1516 , batch Loss= 0.008247, Training Accuracy= 1.00000\n",
      "Epoch   1517 , batch Loss= 0.024596, Training Accuracy= 0.99000\n",
      "Epoch   1518 , batch Loss= 0.015862, Training Accuracy= 1.00000\n",
      "Epoch   1519 , batch Loss= 0.004068, Training Accuracy= 1.00000\n",
      "Epoch   1520 , batch Loss= 0.012194, Training Accuracy= 1.00000\n",
      "Epoch   1521 , batch Loss= 0.001024, Training Accuracy= 1.00000\n",
      "Epoch   1522 , batch Loss= 0.002655, Training Accuracy= 1.00000\n",
      "Epoch   1523 , batch Loss= 0.022163, Training Accuracy= 1.00000\n",
      "Epoch   1524 , batch Loss= 0.024898, Training Accuracy= 0.99000\n",
      "Epoch   1525 , batch Loss= 0.006481, Training Accuracy= 1.00000\n",
      "Epoch   1526 , batch Loss= 0.002563, Training Accuracy= 1.00000\n",
      "Epoch   1527 , batch Loss= 0.013684, Training Accuracy= 0.99000\n",
      "Epoch   1528 , batch Loss= 0.002647, Training Accuracy= 1.00000\n",
      "Epoch   1529 , batch Loss= 0.020129, Training Accuracy= 0.99000\n",
      "Epoch   1530 , batch Loss= 0.038619, Training Accuracy= 0.99000\n",
      "Epoch   1531 , batch Loss= 0.024303, Training Accuracy= 0.98000\n",
      "Epoch   1532 , batch Loss= 0.004697, Training Accuracy= 1.00000\n",
      "Epoch   1533 , batch Loss= 0.012809, Training Accuracy= 0.99000\n",
      "Epoch   1534 , batch Loss= 0.001139, Training Accuracy= 1.00000\n",
      "Epoch   1535 , batch Loss= 0.011722, Training Accuracy= 1.00000\n",
      "Epoch   1536 , batch Loss= 0.022875, Training Accuracy= 0.99000\n",
      "Epoch   1537 , batch Loss= 0.002735, Training Accuracy= 1.00000\n",
      "Epoch   1538 , batch Loss= 0.008173, Training Accuracy= 1.00000\n",
      "Epoch   1539 , batch Loss= 0.009923, Training Accuracy= 1.00000\n",
      "Epoch   1540 , batch Loss= 0.011234, Training Accuracy= 1.00000\n",
      "Epoch   1541 , batch Loss= 0.022547, Training Accuracy= 0.99000\n",
      "Epoch   1542 , batch Loss= 0.027366, Training Accuracy= 0.98000\n",
      "Epoch   1543 , batch Loss= 0.005676, Training Accuracy= 1.00000\n",
      "Epoch   1544 , batch Loss= 0.017445, Training Accuracy= 0.99000\n",
      "Epoch   1545 , batch Loss= 0.005950, Training Accuracy= 1.00000\n",
      "Epoch   1546 , batch Loss= 0.002467, Training Accuracy= 1.00000\n",
      "Epoch   1547 , batch Loss= 0.060872, Training Accuracy= 0.99000\n",
      "Epoch   1548 , batch Loss= 0.002665, Training Accuracy= 1.00000\n",
      "Epoch   1549 , batch Loss= 0.010711, Training Accuracy= 1.00000\n",
      "Epoch   1550 , batch Loss= 0.039088, Training Accuracy= 0.99000\n",
      "Epoch   1551 , batch Loss= 0.017105, Training Accuracy= 0.99000\n",
      "Epoch   1552 , batch Loss= 0.009203, Training Accuracy= 1.00000\n",
      "Epoch   1553 , batch Loss= 0.043801, Training Accuracy= 0.99000\n",
      "Epoch   1554 , batch Loss= 0.003014, Training Accuracy= 1.00000\n",
      "Epoch   1555 , batch Loss= 0.012104, Training Accuracy= 1.00000\n",
      "Epoch   1556 , batch Loss= 0.065806, Training Accuracy= 0.97000\n",
      "Epoch   1557 , batch Loss= 0.010363, Training Accuracy= 1.00000\n",
      "Epoch   1558 , batch Loss= 0.030650, Training Accuracy= 0.98000\n",
      "Epoch   1559 , batch Loss= 0.003886, Training Accuracy= 1.00000\n",
      "Epoch   1560 , batch Loss= 0.006782, Training Accuracy= 1.00000\n",
      "Epoch   1561 , batch Loss= 0.022308, Training Accuracy= 0.99000\n",
      "Epoch   1562 , batch Loss= 0.027191, Training Accuracy= 0.98000\n",
      "Epoch   1563 , batch Loss= 0.003633, Training Accuracy= 1.00000\n",
      "Epoch   1564 , batch Loss= 0.064948, Training Accuracy= 0.98000\n",
      "Epoch   1565 , batch Loss= 0.029624, Training Accuracy= 0.98000\n",
      "Epoch   1566 , batch Loss= 0.033642, Training Accuracy= 0.99000\n",
      "Epoch   1567 , batch Loss= 0.014187, Training Accuracy= 0.99000\n",
      "Epoch   1568 , batch Loss= 0.005515, Training Accuracy= 1.00000\n",
      "Epoch   1569 , batch Loss= 0.012759, Training Accuracy= 1.00000\n",
      "Epoch   1570 , batch Loss= 0.016371, Training Accuracy= 1.00000\n",
      "Epoch   1571 , batch Loss= 0.176785, Training Accuracy= 0.99000\n",
      "Epoch   1572 , batch Loss= 0.002381, Training Accuracy= 1.00000\n",
      "Epoch   1573 , batch Loss= 0.014378, Training Accuracy= 0.99000\n",
      "Epoch   1574 , batch Loss= 0.035815, Training Accuracy= 0.99000\n",
      "Epoch   1575 , batch Loss= 0.005931, Training Accuracy= 1.00000\n",
      "Epoch   1576 , batch Loss= 0.006852, Training Accuracy= 1.00000\n",
      "Epoch   1577 , batch Loss= 0.026769, Training Accuracy= 0.98000\n",
      "Epoch   1578 , batch Loss= 0.098407, Training Accuracy= 0.97000\n",
      "Epoch   1579 , batch Loss= 0.005411, Training Accuracy= 1.00000\n",
      "Epoch   1580 , batch Loss= 0.030098, Training Accuracy= 0.99000\n",
      "Epoch   1581 , batch Loss= 0.009758, Training Accuracy= 1.00000\n",
      "Epoch   1582 , batch Loss= 0.011532, Training Accuracy= 1.00000\n",
      "Epoch   1583 , batch Loss= 0.021088, Training Accuracy= 0.99000\n",
      "Epoch   1584 , batch Loss= 0.019264, Training Accuracy= 0.99000\n",
      "Epoch   1585 , batch Loss= 0.016666, Training Accuracy= 0.99000\n",
      "Epoch   1586 , batch Loss= 0.029976, Training Accuracy= 0.98000\n",
      "Epoch   1587 , batch Loss= 0.012060, Training Accuracy= 1.00000\n",
      "Epoch   1588 , batch Loss= 0.019884, Training Accuracy= 0.99000\n",
      "Epoch   1589 , batch Loss= 0.008373, Training Accuracy= 1.00000\n",
      "Epoch   1590 , batch Loss= 0.026601, Training Accuracy= 0.99000\n",
      "Epoch   1591 , batch Loss= 0.005860, Training Accuracy= 1.00000\n",
      "Epoch   1592 , batch Loss= 0.008583, Training Accuracy= 1.00000\n",
      "Epoch   1593 , batch Loss= 0.016989, Training Accuracy= 0.99000\n",
      "Epoch   1594 , batch Loss= 0.011367, Training Accuracy= 1.00000\n",
      "Epoch   1595 , batch Loss= 0.005355, Training Accuracy= 1.00000\n",
      "Epoch   1596 , batch Loss= 0.010693, Training Accuracy= 1.00000\n",
      "Epoch   1597 , batch Loss= 0.011385, Training Accuracy= 1.00000\n",
      "Epoch   1598 , batch Loss= 0.036350, Training Accuracy= 0.98000\n",
      "Epoch   1599 , batch Loss= 0.002729, Training Accuracy= 1.00000\n",
      "Epoch   1600 , batch Loss= 0.004169, Training Accuracy= 1.00000\n",
      "Epoch   1601 , batch Loss= 0.006390, Training Accuracy= 1.00000\n",
      "Epoch   1602 , batch Loss= 0.009076, Training Accuracy= 1.00000\n",
      "Epoch   1603 , batch Loss= 0.006833, Training Accuracy= 1.00000\n",
      "Epoch   1604 , batch Loss= 0.012861, Training Accuracy= 1.00000\n",
      "Epoch   1605 , batch Loss= 0.032924, Training Accuracy= 0.99000\n",
      "Epoch   1606 , batch Loss= 0.010453, Training Accuracy= 1.00000\n",
      "Epoch   1607 , batch Loss= 0.003881, Training Accuracy= 1.00000\n",
      "Epoch   1608 , batch Loss= 0.005056, Training Accuracy= 1.00000\n",
      "Epoch   1609 , batch Loss= 0.008648, Training Accuracy= 0.99000\n",
      "Epoch   1610 , batch Loss= 0.002760, Training Accuracy= 1.00000\n",
      "Epoch   1611 , batch Loss= 0.028343, Training Accuracy= 0.99000\n",
      "Epoch   1612 , batch Loss= 0.002643, Training Accuracy= 1.00000\n",
      "Epoch   1613 , batch Loss= 0.007293, Training Accuracy= 1.00000\n",
      "Epoch   1614 , batch Loss= 0.016318, Training Accuracy= 0.99000\n",
      "Epoch   1615 , batch Loss= 0.011697, Training Accuracy= 1.00000\n",
      "Epoch   1616 , batch Loss= 0.003616, Training Accuracy= 1.00000\n",
      "Epoch   1617 , batch Loss= 0.008365, Training Accuracy= 1.00000\n",
      "Epoch   1618 , batch Loss= 0.007763, Training Accuracy= 1.00000\n",
      "Epoch   1619 , batch Loss= 0.006396, Training Accuracy= 1.00000\n",
      "Epoch   1620 , batch Loss= 0.019223, Training Accuracy= 0.99000\n",
      "Epoch   1621 , batch Loss= 0.006825, Training Accuracy= 1.00000\n",
      "Epoch   1622 , batch Loss= 0.004620, Training Accuracy= 1.00000\n",
      "Epoch   1623 , batch Loss= 0.009521, Training Accuracy= 1.00000\n",
      "Epoch   1624 , batch Loss= 0.008647, Training Accuracy= 1.00000\n",
      "Epoch   1625 , batch Loss= 0.018023, Training Accuracy= 1.00000\n",
      "Epoch   1626 , batch Loss= 0.034584, Training Accuracy= 0.99000\n",
      "Epoch   1627 , batch Loss= 0.117369, Training Accuracy= 0.97000\n",
      "Epoch   1628 , batch Loss= 0.016077, Training Accuracy= 0.99000\n",
      "Epoch   1629 , batch Loss= 0.023642, Training Accuracy= 0.99000\n",
      "Epoch   1630 , batch Loss= 0.013322, Training Accuracy= 0.99000\n",
      "Epoch   1631 , batch Loss= 0.021801, Training Accuracy= 0.99000\n",
      "Epoch   1632 , batch Loss= 0.039685, Training Accuracy= 0.98000\n",
      "Epoch   1633 , batch Loss= 0.006137, Training Accuracy= 1.00000\n",
      "Epoch   1634 , batch Loss= 0.004973, Training Accuracy= 1.00000\n",
      "Epoch   1635 , batch Loss= 0.006295, Training Accuracy= 1.00000\n",
      "Epoch   1636 , batch Loss= 0.012328, Training Accuracy= 1.00000\n",
      "Epoch   1637 , batch Loss= 0.024506, Training Accuracy= 1.00000\n",
      "Epoch   1638 , batch Loss= 0.012579, Training Accuracy= 0.99000\n",
      "Epoch   1639 , batch Loss= 0.003709, Training Accuracy= 1.00000\n",
      "Epoch   1640 , batch Loss= 0.009671, Training Accuracy= 1.00000\n",
      "Epoch   1641 , batch Loss= 0.020051, Training Accuracy= 0.99000\n",
      "Epoch   1642 , batch Loss= 0.007089, Training Accuracy= 1.00000\n",
      "Epoch   1643 , batch Loss= 0.007833, Training Accuracy= 1.00000\n",
      "Epoch   1644 , batch Loss= 0.053760, Training Accuracy= 0.98000\n",
      "Epoch   1645 , batch Loss= 0.033654, Training Accuracy= 0.99000\n",
      "Epoch   1646 , batch Loss= 0.009906, Training Accuracy= 0.99000\n",
      "Epoch   1647 , batch Loss= 0.005079, Training Accuracy= 1.00000\n",
      "Epoch   1648 , batch Loss= 0.013094, Training Accuracy= 1.00000\n",
      "Epoch   1649 , batch Loss= 0.003554, Training Accuracy= 1.00000\n",
      "Epoch   1650 , batch Loss= 0.015259, Training Accuracy= 0.99000\n",
      "Epoch   1651 , batch Loss= 0.009902, Training Accuracy= 1.00000\n",
      "Epoch   1652 , batch Loss= 0.000668, Training Accuracy= 1.00000\n",
      "Epoch   1653 , batch Loss= 0.032028, Training Accuracy= 0.99000\n",
      "Epoch   1654 , batch Loss= 0.001495, Training Accuracy= 1.00000\n",
      "Epoch   1655 , batch Loss= 0.011122, Training Accuracy= 0.99000\n",
      "Epoch   1656 , batch Loss= 0.011905, Training Accuracy= 1.00000\n",
      "Epoch   1657 , batch Loss= 0.012592, Training Accuracy= 1.00000\n",
      "Epoch   1658 , batch Loss= 0.010011, Training Accuracy= 1.00000\n",
      "Epoch   1659 , batch Loss= 0.007062, Training Accuracy= 1.00000\n",
      "Epoch   1660 , batch Loss= 0.010349, Training Accuracy= 1.00000\n",
      "Epoch   1661 , batch Loss= 0.004040, Training Accuracy= 1.00000\n",
      "Epoch   1662 , batch Loss= 0.007702, Training Accuracy= 1.00000\n",
      "Epoch   1663 , batch Loss= 0.001641, Training Accuracy= 1.00000\n",
      "Epoch   1664 , batch Loss= 0.002712, Training Accuracy= 1.00000\n",
      "Epoch   1665 , batch Loss= 0.013101, Training Accuracy= 1.00000\n",
      "Epoch   1666 , batch Loss= 0.004195, Training Accuracy= 1.00000\n",
      "Epoch   1667 , batch Loss= 0.022106, Training Accuracy= 0.99000\n",
      "Epoch   1668 , batch Loss= 0.008580, Training Accuracy= 1.00000\n",
      "Epoch   1669 , batch Loss= 0.007894, Training Accuracy= 1.00000\n",
      "Epoch   1670 , batch Loss= 0.007126, Training Accuracy= 1.00000\n",
      "Epoch   1671 , batch Loss= 0.003812, Training Accuracy= 1.00000\n",
      "Epoch   1672 , batch Loss= 0.001199, Training Accuracy= 1.00000\n",
      "Epoch   1673 , batch Loss= 0.001797, Training Accuracy= 1.00000\n",
      "Epoch   1674 , batch Loss= 0.001940, Training Accuracy= 1.00000\n",
      "Epoch   1675 , batch Loss= 0.035286, Training Accuracy= 0.99000\n",
      "Epoch   1676 , batch Loss= 0.025624, Training Accuracy= 0.99000\n",
      "Epoch   1677 , batch Loss= 0.001752, Training Accuracy= 1.00000\n",
      "Epoch   1678 , batch Loss= 0.026018, Training Accuracy= 0.98000\n",
      "Epoch   1679 , batch Loss= 0.004746, Training Accuracy= 1.00000\n",
      "Epoch   1680 , batch Loss= 0.003354, Training Accuracy= 1.00000\n",
      "Epoch   1681 , batch Loss= 0.012358, Training Accuracy= 1.00000\n",
      "Epoch   1682 , batch Loss= 0.011120, Training Accuracy= 0.99000\n",
      "Epoch   1683 , batch Loss= 0.001229, Training Accuracy= 1.00000\n",
      "Epoch   1684 , batch Loss= 0.011988, Training Accuracy= 0.99000\n",
      "Epoch   1685 , batch Loss= 0.019054, Training Accuracy= 0.99000\n",
      "Epoch   1686 , batch Loss= 0.002818, Training Accuracy= 1.00000\n",
      "Epoch   1687 , batch Loss= 0.013296, Training Accuracy= 0.99000\n",
      "Epoch   1688 , batch Loss= 0.018838, Training Accuracy= 0.99000\n",
      "Epoch   1689 , batch Loss= 0.002583, Training Accuracy= 1.00000\n",
      "Epoch   1690 , batch Loss= 0.003469, Training Accuracy= 1.00000\n",
      "Epoch   1691 , batch Loss= 0.010499, Training Accuracy= 0.99000\n",
      "Epoch   1692 , batch Loss= 0.001914, Training Accuracy= 1.00000\n",
      "Epoch   1693 , batch Loss= 0.002844, Training Accuracy= 1.00000\n",
      "Epoch   1694 , batch Loss= 0.014706, Training Accuracy= 1.00000\n",
      "Epoch   1695 , batch Loss= 0.001206, Training Accuracy= 1.00000\n",
      "Epoch   1696 , batch Loss= 0.025209, Training Accuracy= 0.99000\n",
      "Epoch   1697 , batch Loss= 0.006975, Training Accuracy= 1.00000\n",
      "Epoch   1698 , batch Loss= 0.018517, Training Accuracy= 0.99000\n",
      "Epoch   1699 , batch Loss= 0.024224, Training Accuracy= 0.99000\n",
      "Epoch   1700 , batch Loss= 0.002802, Training Accuracy= 1.00000\n",
      "Epoch   1701 , batch Loss= 0.009618, Training Accuracy= 1.00000\n",
      "Epoch   1702 , batch Loss= 0.002685, Training Accuracy= 1.00000\n",
      "Epoch   1703 , batch Loss= 0.002640, Training Accuracy= 1.00000\n",
      "Epoch   1704 , batch Loss= 0.004590, Training Accuracy= 1.00000\n",
      "Epoch   1705 , batch Loss= 0.001027, Training Accuracy= 1.00000\n",
      "Epoch   1706 , batch Loss= 0.002525, Training Accuracy= 1.00000\n",
      "Epoch   1707 , batch Loss= 0.005957, Training Accuracy= 1.00000\n",
      "Epoch   1708 , batch Loss= 0.002106, Training Accuracy= 1.00000\n",
      "Epoch   1709 , batch Loss= 0.003306, Training Accuracy= 1.00000\n",
      "Epoch   1710 , batch Loss= 0.005567, Training Accuracy= 1.00000\n",
      "Epoch   1711 , batch Loss= 0.004032, Training Accuracy= 1.00000\n",
      "Epoch   1712 , batch Loss= 0.004043, Training Accuracy= 1.00000\n",
      "Epoch   1713 , batch Loss= 0.002525, Training Accuracy= 1.00000\n",
      "Epoch   1714 , batch Loss= 0.002317, Training Accuracy= 1.00000\n",
      "Epoch   1715 , batch Loss= 0.003083, Training Accuracy= 1.00000\n",
      "Epoch   1716 , batch Loss= 0.003858, Training Accuracy= 1.00000\n",
      "Epoch   1717 , batch Loss= 0.004379, Training Accuracy= 1.00000\n",
      "Epoch   1718 , batch Loss= 0.014784, Training Accuracy= 0.99000\n",
      "Epoch   1719 , batch Loss= 0.003313, Training Accuracy= 1.00000\n",
      "Epoch   1720 , batch Loss= 0.010502, Training Accuracy= 1.00000\n",
      "Epoch   1721 , batch Loss= 0.004946, Training Accuracy= 1.00000\n",
      "Epoch   1722 , batch Loss= 0.018019, Training Accuracy= 1.00000\n",
      "Epoch   1723 , batch Loss= 0.003128, Training Accuracy= 1.00000\n",
      "Epoch   1724 , batch Loss= 0.013179, Training Accuracy= 1.00000\n",
      "Epoch   1725 , batch Loss= 0.002608, Training Accuracy= 1.00000\n",
      "Epoch   1726 , batch Loss= 0.003282, Training Accuracy= 1.00000\n",
      "Epoch   1727 , batch Loss= 0.003207, Training Accuracy= 1.00000\n",
      "Epoch   1728 , batch Loss= 0.012737, Training Accuracy= 0.99000\n",
      "Epoch   1729 , batch Loss= 0.003438, Training Accuracy= 1.00000\n",
      "Epoch   1730 , batch Loss= 0.002014, Training Accuracy= 1.00000\n",
      "Epoch   1731 , batch Loss= 0.018152, Training Accuracy= 0.99000\n",
      "Epoch   1732 , batch Loss= 0.001695, Training Accuracy= 1.00000\n",
      "Epoch   1733 , batch Loss= 0.008715, Training Accuracy= 0.99000\n",
      "Epoch   1734 , batch Loss= 0.006650, Training Accuracy= 1.00000\n",
      "Epoch   1735 , batch Loss= 0.001748, Training Accuracy= 1.00000\n",
      "Epoch   1736 , batch Loss= 0.000284, Training Accuracy= 1.00000\n",
      "Epoch   1737 , batch Loss= 0.003534, Training Accuracy= 1.00000\n",
      "Epoch   1738 , batch Loss= 0.000495, Training Accuracy= 1.00000\n",
      "Epoch   1739 , batch Loss= 0.002791, Training Accuracy= 1.00000\n",
      "Epoch   1740 , batch Loss= 0.008446, Training Accuracy= 0.99000\n",
      "Epoch   1741 , batch Loss= 0.001131, Training Accuracy= 1.00000\n",
      "Epoch   1742 , batch Loss= 0.005600, Training Accuracy= 1.00000\n",
      "Epoch   1743 , batch Loss= 0.004090, Training Accuracy= 1.00000\n",
      "Epoch   1744 , batch Loss= 0.000397, Training Accuracy= 1.00000\n",
      "Epoch   1745 , batch Loss= 0.017603, Training Accuracy= 0.99000\n",
      "Epoch   1746 , batch Loss= 0.016139, Training Accuracy= 0.99000\n",
      "Epoch   1747 , batch Loss= 0.014515, Training Accuracy= 0.99000\n",
      "Epoch   1748 , batch Loss= 0.019557, Training Accuracy= 0.99000\n",
      "Epoch   1749 , batch Loss= 0.003573, Training Accuracy= 1.00000\n",
      "Epoch   1750 , batch Loss= 0.001512, Training Accuracy= 1.00000\n",
      "Epoch   1751 , batch Loss= 0.003183, Training Accuracy= 1.00000\n",
      "Epoch   1752 , batch Loss= 0.008566, Training Accuracy= 1.00000\n",
      "Epoch   1753 , batch Loss= 0.006504, Training Accuracy= 1.00000\n",
      "Epoch   1754 , batch Loss= 0.005843, Training Accuracy= 1.00000\n",
      "Epoch   1755 , batch Loss= 0.030898, Training Accuracy= 0.98000\n",
      "Epoch   1756 , batch Loss= 0.030088, Training Accuracy= 0.99000\n",
      "Epoch   1757 , batch Loss= 0.002158, Training Accuracy= 1.00000\n",
      "Epoch   1758 , batch Loss= 0.004837, Training Accuracy= 1.00000\n",
      "Epoch   1759 , batch Loss= 0.006281, Training Accuracy= 1.00000\n",
      "Epoch   1760 , batch Loss= 0.002519, Training Accuracy= 1.00000\n",
      "Epoch   1761 , batch Loss= 0.007212, Training Accuracy= 1.00000\n",
      "Epoch   1762 , batch Loss= 0.003950, Training Accuracy= 1.00000\n",
      "Epoch   1763 , batch Loss= 0.000950, Training Accuracy= 1.00000\n",
      "Epoch   1764 , batch Loss= 0.015619, Training Accuracy= 0.99000\n",
      "Epoch   1765 , batch Loss= 0.001663, Training Accuracy= 1.00000\n",
      "Epoch   1766 , batch Loss= 0.003907, Training Accuracy= 1.00000\n",
      "Epoch   1767 , batch Loss= 0.014864, Training Accuracy= 0.99000\n",
      "Epoch   1768 , batch Loss= 0.006635, Training Accuracy= 1.00000\n",
      "Epoch   1769 , batch Loss= 0.002666, Training Accuracy= 1.00000\n",
      "Epoch   1770 , batch Loss= 0.001880, Training Accuracy= 1.00000\n",
      "Epoch   1771 , batch Loss= 0.001296, Training Accuracy= 1.00000\n",
      "Epoch   1772 , batch Loss= 0.010884, Training Accuracy= 1.00000\n",
      "Epoch   1773 , batch Loss= 0.006278, Training Accuracy= 1.00000\n",
      "Epoch   1774 , batch Loss= 0.000614, Training Accuracy= 1.00000\n",
      "Epoch   1775 , batch Loss= 0.003396, Training Accuracy= 1.00000\n",
      "Epoch   1776 , batch Loss= 0.001997, Training Accuracy= 1.00000\n",
      "Epoch   1777 , batch Loss= 0.000325, Training Accuracy= 1.00000\n",
      "Epoch   1778 , batch Loss= 0.001833, Training Accuracy= 1.00000\n",
      "Epoch   1779 , batch Loss= 0.001592, Training Accuracy= 1.00000\n",
      "Epoch   1780 , batch Loss= 0.010390, Training Accuracy= 0.99000\n",
      "Epoch   1781 , batch Loss= 0.037984, Training Accuracy= 0.99000\n",
      "Epoch   1782 , batch Loss= 0.005234, Training Accuracy= 1.00000\n",
      "Epoch   1783 , batch Loss= 0.008460, Training Accuracy= 0.99000\n",
      "Epoch   1784 , batch Loss= 0.002232, Training Accuracy= 1.00000\n",
      "Epoch   1785 , batch Loss= 0.012689, Training Accuracy= 0.99000\n",
      "Epoch   1786 , batch Loss= 0.027679, Training Accuracy= 0.99000\n",
      "Epoch   1787 , batch Loss= 0.005599, Training Accuracy= 1.00000\n",
      "Epoch   1788 , batch Loss= 0.002408, Training Accuracy= 1.00000\n",
      "Epoch   1789 , batch Loss= 0.017692, Training Accuracy= 0.99000\n",
      "Epoch   1790 , batch Loss= 0.024930, Training Accuracy= 0.99000\n",
      "Epoch   1791 , batch Loss= 0.000940, Training Accuracy= 1.00000\n",
      "Epoch   1792 , batch Loss= 0.000991, Training Accuracy= 1.00000\n",
      "Epoch   1793 , batch Loss= 0.008577, Training Accuracy= 1.00000\n",
      "Epoch   1794 , batch Loss= 0.002267, Training Accuracy= 1.00000\n",
      "Epoch   1795 , batch Loss= 0.001986, Training Accuracy= 1.00000\n",
      "Epoch   1796 , batch Loss= 0.025268, Training Accuracy= 0.99000\n",
      "Epoch   1797 , batch Loss= 0.008771, Training Accuracy= 0.99000\n",
      "Epoch   1798 , batch Loss= 0.005326, Training Accuracy= 1.00000\n",
      "Epoch   1799 , batch Loss= 0.002997, Training Accuracy= 1.00000\n",
      "Epoch   1800 , batch Loss= 0.013831, Training Accuracy= 0.99000\n",
      "Epoch   1801 , batch Loss= 0.002358, Training Accuracy= 1.00000\n",
      "Epoch   1802 , batch Loss= 0.008470, Training Accuracy= 1.00000\n",
      "Epoch   1803 , batch Loss= 0.005620, Training Accuracy= 1.00000\n",
      "Epoch   1804 , batch Loss= 0.035363, Training Accuracy= 0.99000\n",
      "Epoch   1805 , batch Loss= 0.006125, Training Accuracy= 1.00000\n",
      "Epoch   1806 , batch Loss= 0.003212, Training Accuracy= 1.00000\n",
      "Epoch   1807 , batch Loss= 0.013386, Training Accuracy= 0.99000\n",
      "Epoch   1808 , batch Loss= 0.001464, Training Accuracy= 1.00000\n",
      "Epoch   1809 , batch Loss= 0.000681, Training Accuracy= 1.00000\n",
      "Epoch   1810 , batch Loss= 0.000938, Training Accuracy= 1.00000\n",
      "Epoch   1811 , batch Loss= 0.001733, Training Accuracy= 1.00000\n",
      "Epoch   1812 , batch Loss= 0.003369, Training Accuracy= 1.00000\n",
      "Epoch   1813 , batch Loss= 0.002249, Training Accuracy= 1.00000\n",
      "Epoch   1814 , batch Loss= 0.001659, Training Accuracy= 1.00000\n",
      "Epoch   1815 , batch Loss= 0.026055, Training Accuracy= 0.99000\n",
      "Epoch   1816 , batch Loss= 0.015509, Training Accuracy= 0.99000\n",
      "Epoch   1817 , batch Loss= 0.003023, Training Accuracy= 1.00000\n",
      "Epoch   1818 , batch Loss= 0.000962, Training Accuracy= 1.00000\n",
      "Epoch   1819 , batch Loss= 0.002387, Training Accuracy= 1.00000\n",
      "Epoch   1820 , batch Loss= 0.001665, Training Accuracy= 1.00000\n",
      "Epoch   1821 , batch Loss= 0.006001, Training Accuracy= 1.00000\n",
      "Epoch   1822 , batch Loss= 0.003579, Training Accuracy= 1.00000\n",
      "Epoch   1823 , batch Loss= 0.005390, Training Accuracy= 1.00000\n",
      "Epoch   1824 , batch Loss= 0.008915, Training Accuracy= 1.00000\n",
      "Epoch   1825 , batch Loss= 0.032614, Training Accuracy= 0.98000\n",
      "Epoch   1826 , batch Loss= 0.002499, Training Accuracy= 1.00000\n",
      "Epoch   1827 , batch Loss= 0.006270, Training Accuracy= 1.00000\n",
      "Epoch   1828 , batch Loss= 0.004174, Training Accuracy= 1.00000\n",
      "Epoch   1829 , batch Loss= 0.003326, Training Accuracy= 1.00000\n",
      "Epoch   1830 , batch Loss= 0.005585, Training Accuracy= 1.00000\n",
      "Epoch   1831 , batch Loss= 0.007266, Training Accuracy= 1.00000\n",
      "Epoch   1832 , batch Loss= 0.004730, Training Accuracy= 1.00000\n",
      "Epoch   1833 , batch Loss= 0.003856, Training Accuracy= 1.00000\n",
      "Epoch   1834 , batch Loss= 0.001247, Training Accuracy= 1.00000\n",
      "Epoch   1835 , batch Loss= 0.014900, Training Accuracy= 1.00000\n",
      "Epoch   1836 , batch Loss= 0.002709, Training Accuracy= 1.00000\n",
      "Epoch   1837 , batch Loss= 0.006441, Training Accuracy= 1.00000\n",
      "Epoch   1838 , batch Loss= 0.007619, Training Accuracy= 1.00000\n",
      "Epoch   1839 , batch Loss= 0.004652, Training Accuracy= 1.00000\n",
      "Epoch   1840 , batch Loss= 0.004464, Training Accuracy= 1.00000\n",
      "Epoch   1841 , batch Loss= 0.024668, Training Accuracy= 0.99000\n",
      "Epoch   1842 , batch Loss= 0.037815, Training Accuracy= 0.99000\n",
      "Epoch   1843 , batch Loss= 0.001613, Training Accuracy= 1.00000\n",
      "Epoch   1844 , batch Loss= 0.003087, Training Accuracy= 1.00000\n",
      "Epoch   1845 , batch Loss= 0.013288, Training Accuracy= 0.99000\n",
      "Epoch   1846 , batch Loss= 0.003075, Training Accuracy= 1.00000\n",
      "Epoch   1847 , batch Loss= 0.012115, Training Accuracy= 0.99000\n",
      "Epoch   1848 , batch Loss= 0.075330, Training Accuracy= 0.99000\n",
      "Epoch   1849 , batch Loss= 0.007142, Training Accuracy= 1.00000\n",
      "Epoch   1850 , batch Loss= 0.003853, Training Accuracy= 1.00000\n",
      "Epoch   1851 , batch Loss= 0.019933, Training Accuracy= 0.99000\n",
      "Epoch   1852 , batch Loss= 0.005603, Training Accuracy= 1.00000\n",
      "Epoch   1853 , batch Loss= 0.003355, Training Accuracy= 1.00000\n",
      "Epoch   1854 , batch Loss= 0.004427, Training Accuracy= 1.00000\n",
      "Epoch   1855 , batch Loss= 0.004239, Training Accuracy= 1.00000\n",
      "Epoch   1856 , batch Loss= 0.020734, Training Accuracy= 0.99000\n",
      "Epoch   1857 , batch Loss= 0.003547, Training Accuracy= 1.00000\n",
      "Epoch   1858 , batch Loss= 0.006062, Training Accuracy= 1.00000\n",
      "Epoch   1859 , batch Loss= 0.011682, Training Accuracy= 0.99000\n",
      "Epoch   1860 , batch Loss= 0.003803, Training Accuracy= 1.00000\n",
      "Epoch   1861 , batch Loss= 0.005673, Training Accuracy= 1.00000\n",
      "Epoch   1862 , batch Loss= 0.007899, Training Accuracy= 1.00000\n",
      "Epoch   1863 , batch Loss= 0.012540, Training Accuracy= 0.99000\n",
      "Epoch   1864 , batch Loss= 0.010535, Training Accuracy= 1.00000\n",
      "Epoch   1865 , batch Loss= 0.064198, Training Accuracy= 0.99000\n",
      "Epoch   1866 , batch Loss= 0.009891, Training Accuracy= 1.00000\n",
      "Epoch   1867 , batch Loss= 0.007115, Training Accuracy= 1.00000\n",
      "Epoch   1868 , batch Loss= 0.002106, Training Accuracy= 1.00000\n",
      "Epoch   1869 , batch Loss= 0.027940, Training Accuracy= 0.99000\n",
      "Epoch   1870 , batch Loss= 0.004426, Training Accuracy= 1.00000\n",
      "Epoch   1871 , batch Loss= 0.004932, Training Accuracy= 1.00000\n",
      "Epoch   1872 , batch Loss= 0.006367, Training Accuracy= 1.00000\n",
      "Epoch   1873 , batch Loss= 0.010496, Training Accuracy= 1.00000\n",
      "Epoch   1874 , batch Loss= 0.001167, Training Accuracy= 1.00000\n",
      "Epoch   1875 , batch Loss= 0.002190, Training Accuracy= 1.00000\n",
      "Epoch   1876 , batch Loss= 0.003735, Training Accuracy= 1.00000\n",
      "Epoch   1877 , batch Loss= 0.005827, Training Accuracy= 1.00000\n",
      "Epoch   1878 , batch Loss= 0.004239, Training Accuracy= 1.00000\n",
      "Epoch   1879 , batch Loss= 0.008904, Training Accuracy= 1.00000\n",
      "Epoch   1880 , batch Loss= 0.058157, Training Accuracy= 0.99000\n",
      "Epoch   1881 , batch Loss= 0.001007, Training Accuracy= 1.00000\n",
      "Epoch   1882 , batch Loss= 0.004507, Training Accuracy= 1.00000\n",
      "Epoch   1883 , batch Loss= 0.025146, Training Accuracy= 0.99000\n",
      "Epoch   1884 , batch Loss= 0.003170, Training Accuracy= 1.00000\n",
      "Epoch   1885 , batch Loss= 0.003821, Training Accuracy= 1.00000\n",
      "Epoch   1886 , batch Loss= 0.012709, Training Accuracy= 1.00000\n",
      "Epoch   1887 , batch Loss= 0.050776, Training Accuracy= 0.98000\n",
      "Epoch   1888 , batch Loss= 0.046683, Training Accuracy= 0.98000\n",
      "Epoch   1889 , batch Loss= 0.004674, Training Accuracy= 1.00000\n",
      "Epoch   1890 , batch Loss= 0.002558, Training Accuracy= 1.00000\n",
      "Epoch   1891 , batch Loss= 0.000522, Training Accuracy= 1.00000\n",
      "Epoch   1892 , batch Loss= 0.000312, Training Accuracy= 1.00000\n",
      "Epoch   1893 , batch Loss= 0.016063, Training Accuracy= 0.99000\n",
      "Epoch   1894 , batch Loss= 0.006944, Training Accuracy= 1.00000\n",
      "Epoch   1895 , batch Loss= 0.008440, Training Accuracy= 1.00000\n",
      "Epoch   1896 , batch Loss= 0.004841, Training Accuracy= 1.00000\n",
      "Epoch   1897 , batch Loss= 0.001963, Training Accuracy= 1.00000\n",
      "Epoch   1898 , batch Loss= 0.012619, Training Accuracy= 0.99000\n",
      "Epoch   1899 , batch Loss= 0.005943, Training Accuracy= 1.00000\n",
      "Epoch   1900 , batch Loss= 0.016752, Training Accuracy= 0.99000\n",
      "Epoch   1901 , batch Loss= 0.001944, Training Accuracy= 1.00000\n",
      "Epoch   1902 , batch Loss= 0.004940, Training Accuracy= 1.00000\n",
      "Epoch   1903 , batch Loss= 0.004174, Training Accuracy= 1.00000\n",
      "Epoch   1904 , batch Loss= 0.002140, Training Accuracy= 1.00000\n",
      "Epoch   1905 , batch Loss= 0.004370, Training Accuracy= 1.00000\n",
      "Epoch   1906 , batch Loss= 0.001068, Training Accuracy= 1.00000\n",
      "Epoch   1907 , batch Loss= 0.006053, Training Accuracy= 1.00000\n",
      "Epoch   1908 , batch Loss= 0.010058, Training Accuracy= 1.00000\n",
      "Epoch   1909 , batch Loss= 0.005882, Training Accuracy= 1.00000\n",
      "Epoch   1910 , batch Loss= 0.010893, Training Accuracy= 1.00000\n",
      "Epoch   1911 , batch Loss= 0.004652, Training Accuracy= 1.00000\n",
      "Epoch   1912 , batch Loss= 0.001760, Training Accuracy= 1.00000\n",
      "Epoch   1913 , batch Loss= 0.001294, Training Accuracy= 1.00000\n",
      "Epoch   1914 , batch Loss= 0.016873, Training Accuracy= 0.99000\n",
      "Epoch   1915 , batch Loss= 0.000998, Training Accuracy= 1.00000\n",
      "Epoch   1916 , batch Loss= 0.007455, Training Accuracy= 1.00000\n",
      "Epoch   1917 , batch Loss= 0.003651, Training Accuracy= 1.00000\n",
      "Epoch   1918 , batch Loss= 0.015109, Training Accuracy= 0.99000\n",
      "Epoch   1919 , batch Loss= 0.001901, Training Accuracy= 1.00000\n",
      "Epoch   1920 , batch Loss= 0.003476, Training Accuracy= 1.00000\n",
      "Epoch   1921 , batch Loss= 0.002848, Training Accuracy= 1.00000\n",
      "Epoch   1922 , batch Loss= 0.001269, Training Accuracy= 1.00000\n",
      "Epoch   1923 , batch Loss= 0.001694, Training Accuracy= 1.00000\n",
      "Epoch   1924 , batch Loss= 0.003929, Training Accuracy= 1.00000\n",
      "Epoch   1925 , batch Loss= 0.003466, Training Accuracy= 1.00000\n",
      "Epoch   1926 , batch Loss= 0.020898, Training Accuracy= 0.99000\n",
      "Epoch   1927 , batch Loss= 0.006240, Training Accuracy= 1.00000\n",
      "Epoch   1928 , batch Loss= 0.002822, Training Accuracy= 1.00000\n",
      "Epoch   1929 , batch Loss= 0.003468, Training Accuracy= 1.00000\n",
      "Epoch   1930 , batch Loss= 0.005349, Training Accuracy= 1.00000\n",
      "Epoch   1931 , batch Loss= 0.006470, Training Accuracy= 1.00000\n",
      "Epoch   1932 , batch Loss= 0.017139, Training Accuracy= 0.99000\n",
      "Epoch   1933 , batch Loss= 0.010464, Training Accuracy= 1.00000\n",
      "Epoch   1934 , batch Loss= 0.011262, Training Accuracy= 1.00000\n",
      "Epoch   1935 , batch Loss= 0.008505, Training Accuracy= 1.00000\n",
      "Epoch   1936 , batch Loss= 0.004941, Training Accuracy= 1.00000\n",
      "Epoch   1937 , batch Loss= 0.002125, Training Accuracy= 1.00000\n",
      "Epoch   1938 , batch Loss= 0.001637, Training Accuracy= 1.00000\n",
      "Epoch   1939 , batch Loss= 0.015777, Training Accuracy= 0.99000\n",
      "Epoch   1940 , batch Loss= 0.001911, Training Accuracy= 1.00000\n",
      "Epoch   1941 , batch Loss= 0.000477, Training Accuracy= 1.00000\n",
      "Epoch   1942 , batch Loss= 0.006823, Training Accuracy= 1.00000\n",
      "Epoch   1943 , batch Loss= 0.003625, Training Accuracy= 1.00000\n",
      "Epoch   1944 , batch Loss= 0.001041, Training Accuracy= 1.00000\n",
      "Epoch   1945 , batch Loss= 0.005262, Training Accuracy= 1.00000\n",
      "Epoch   1946 , batch Loss= 0.000588, Training Accuracy= 1.00000\n",
      "Epoch   1947 , batch Loss= 0.000903, Training Accuracy= 1.00000\n",
      "Epoch   1948 , batch Loss= 0.012056, Training Accuracy= 0.99000\n",
      "Epoch   1949 , batch Loss= 0.005000, Training Accuracy= 1.00000\n",
      "Epoch   1950 , batch Loss= 0.002808, Training Accuracy= 1.00000\n",
      "Epoch   1951 , batch Loss= 0.005544, Training Accuracy= 1.00000\n",
      "Epoch   1952 , batch Loss= 0.032574, Training Accuracy= 0.98000\n",
      "Epoch   1953 , batch Loss= 0.016901, Training Accuracy= 0.99000\n",
      "Epoch   1954 , batch Loss= 0.001030, Training Accuracy= 1.00000\n",
      "Epoch   1955 , batch Loss= 0.000467, Training Accuracy= 1.00000\n",
      "Epoch   1956 , batch Loss= 0.002190, Training Accuracy= 1.00000\n",
      "Epoch   1957 , batch Loss= 0.004252, Training Accuracy= 1.00000\n",
      "Epoch   1958 , batch Loss= 0.009421, Training Accuracy= 1.00000\n",
      "Epoch   1959 , batch Loss= 0.001654, Training Accuracy= 1.00000\n",
      "Epoch   1960 , batch Loss= 0.002291, Training Accuracy= 1.00000\n",
      "Epoch   1961 , batch Loss= 0.003173, Training Accuracy= 1.00000\n",
      "Epoch   1962 , batch Loss= 0.003716, Training Accuracy= 1.00000\n",
      "Epoch   1963 , batch Loss= 0.000999, Training Accuracy= 1.00000\n",
      "Epoch   1964 , batch Loss= 0.003127, Training Accuracy= 1.00000\n",
      "Epoch   1965 , batch Loss= 0.003757, Training Accuracy= 1.00000\n",
      "Epoch   1966 , batch Loss= 0.000809, Training Accuracy= 1.00000\n",
      "Epoch   1967 , batch Loss= 0.010320, Training Accuracy= 1.00000\n",
      "Epoch   1968 , batch Loss= 0.014741, Training Accuracy= 0.99000\n",
      "Epoch   1969 , batch Loss= 0.002888, Training Accuracy= 1.00000\n",
      "Epoch   1970 , batch Loss= 0.017837, Training Accuracy= 0.99000\n",
      "Epoch   1971 , batch Loss= 0.002400, Training Accuracy= 1.00000\n",
      "Epoch   1972 , batch Loss= 0.003725, Training Accuracy= 1.00000\n",
      "Epoch   1973 , batch Loss= 0.013984, Training Accuracy= 0.99000\n",
      "Epoch   1974 , batch Loss= 0.013461, Training Accuracy= 1.00000\n",
      "Epoch   1975 , batch Loss= 0.018242, Training Accuracy= 1.00000\n",
      "Epoch   1976 , batch Loss= 0.008460, Training Accuracy= 1.00000\n",
      "Epoch   1977 , batch Loss= 0.003260, Training Accuracy= 1.00000\n",
      "Epoch   1978 , batch Loss= 0.001376, Training Accuracy= 1.00000\n",
      "Epoch   1979 , batch Loss= 0.012030, Training Accuracy= 0.99000\n",
      "Epoch   1980 , batch Loss= 0.019374, Training Accuracy= 0.99000\n",
      "Epoch   1981 , batch Loss= 0.001116, Training Accuracy= 1.00000\n",
      "Epoch   1982 , batch Loss= 0.004872, Training Accuracy= 1.00000\n",
      "Epoch   1983 , batch Loss= 0.000216, Training Accuracy= 1.00000\n",
      "Epoch   1984 , batch Loss= 0.001294, Training Accuracy= 1.00000\n",
      "Epoch   1985 , batch Loss= 0.009616, Training Accuracy= 0.99000\n",
      "Epoch   1986 , batch Loss= 0.023725, Training Accuracy= 0.99000\n",
      "Epoch   1987 , batch Loss= 0.003147, Training Accuracy= 1.00000\n",
      "Epoch   1988 , batch Loss= 0.005173, Training Accuracy= 1.00000\n",
      "Epoch   1989 , batch Loss= 0.002894, Training Accuracy= 1.00000\n",
      "Epoch   1990 , batch Loss= 0.002245, Training Accuracy= 1.00000\n",
      "Epoch   1991 , batch Loss= 0.048465, Training Accuracy= 0.98000\n",
      "Epoch   1992 , batch Loss= 0.024906, Training Accuracy= 0.99000\n",
      "Epoch   1993 , batch Loss= 0.012490, Training Accuracy= 0.99000\n",
      "Epoch   1994 , batch Loss= 0.007710, Training Accuracy= 1.00000\n",
      "Epoch   1995 , batch Loss= 0.000727, Training Accuracy= 1.00000\n",
      "Epoch   1996 , batch Loss= 0.004192, Training Accuracy= 1.00000\n",
      "Epoch   1997 , batch Loss= 0.001013, Training Accuracy= 1.00000\n",
      "Epoch   1998 , batch Loss= 0.002012, Training Accuracy= 1.00000\n",
      "Epoch   1999 , batch Loss= 0.001839, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "            # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,keep_prob: 1.\n",
    "                                                          })\n",
    "        print (\"Epoch  \" ,step  ,\", batch Loss= \" + \\\n",
    "              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc))\n",
    "        step +=1\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy mnist test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                      y: mnist.test.labels,\n",
    "                                      keep_prob: 1.\n",
    "                                      }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
