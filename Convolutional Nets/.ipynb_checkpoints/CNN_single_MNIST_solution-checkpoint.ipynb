{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "n_input_width = 28\n",
    "n_input_height = 28\n",
    "n_input_channel = 1\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_conv1_patch_size = 3 \n",
    "n_conv1_filter = 84\n",
    "\n",
    "n_output  = 10 # e.g. MNIST total classes (0-9 digits)\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input_width * n_input_height])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "\n",
    "wc1 = tf.Variable(tf.random_normal([n_conv1_patch_size, n_conv1_patch_size, n_input_channel, n_conv1_filter], stddev=0.1))\n",
    "bc1 = tf.Variable(tf.random_normal([n_conv1_filter], stddev=0.1))\n",
    "\n",
    "wf1 = tf.Variable(tf.random_normal([(n_input_width/2)*(n_input_height/2)*n_conv1_filter, n_output], stddev=0.1))\n",
    "bf1 = tf.Variable(tf.random_normal([n_output], stddev=0.1))\n",
    "\n",
    "# Reshape input\n",
    "input_r = tf.reshape(x, shape=[-1, n_input_width, n_input_width, 1])\n",
    "# Convolution\n",
    "conv = tf.nn.conv2d(input_r, wc1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# Add-bias\n",
    "bias = tf.nn.bias_add(conv, bc1)\n",
    "# Pass ReLu\n",
    "relu = tf.nn.relu(bias)\n",
    "# Max-pooling\n",
    "pool  = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Vectorize\n",
    "dense = tf.reshape(pool, [-1, wf1.get_shape().as_list()[0]])\n",
    "# Fully-connected layer\n",
    "out = tf.add(tf.matmul(dense, wf1), bf1)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(out, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(out, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 , batch Loss= 7.941833, Training Accuracy= 0.31000\n",
      "Epoch   1 , batch Loss= 8.856786, Training Accuracy= 0.20000\n",
      "Epoch   2 , batch Loss= 6.707122, Training Accuracy= 0.17000\n",
      "Epoch   3 , batch Loss= 5.313335, Training Accuracy= 0.35000\n",
      "Epoch   4 , batch Loss= 5.474058, Training Accuracy= 0.24000\n",
      "Epoch   5 , batch Loss= 3.754120, Training Accuracy= 0.22000\n",
      "Epoch   6 , batch Loss= 2.175781, Training Accuracy= 0.40000\n",
      "Epoch   7 , batch Loss= 1.830510, Training Accuracy= 0.53000\n",
      "Epoch   8 , batch Loss= 2.065017, Training Accuracy= 0.41000\n",
      "Epoch   9 , batch Loss= 2.071018, Training Accuracy= 0.47000\n",
      "Epoch   10 , batch Loss= 2.058568, Training Accuracy= 0.50000\n",
      "Epoch   11 , batch Loss= 1.651686, Training Accuracy= 0.51000\n",
      "Epoch   12 , batch Loss= 1.508850, Training Accuracy= 0.52000\n",
      "Epoch   13 , batch Loss= 0.977446, Training Accuracy= 0.79000\n",
      "Epoch   14 , batch Loss= 0.852679, Training Accuracy= 0.75000\n",
      "Epoch   15 , batch Loss= 0.502403, Training Accuracy= 0.85000\n",
      "Epoch   16 , batch Loss= 0.597501, Training Accuracy= 0.82000\n",
      "Epoch   17 , batch Loss= 0.522443, Training Accuracy= 0.89000\n",
      "Epoch   18 , batch Loss= 0.812815, Training Accuracy= 0.77000\n",
      "Epoch   19 , batch Loss= 0.538282, Training Accuracy= 0.82000\n",
      "Epoch   20 , batch Loss= 0.577422, Training Accuracy= 0.82000\n",
      "Epoch   21 , batch Loss= 0.526602, Training Accuracy= 0.82000\n",
      "Epoch   22 , batch Loss= 0.656280, Training Accuracy= 0.80000\n",
      "Epoch   23 , batch Loss= 0.436648, Training Accuracy= 0.88000\n",
      "Epoch   24 , batch Loss= 0.383745, Training Accuracy= 0.88000\n",
      "Epoch   25 , batch Loss= 0.365410, Training Accuracy= 0.87000\n",
      "Epoch   26 , batch Loss= 0.467611, Training Accuracy= 0.84000\n",
      "Epoch   27 , batch Loss= 0.483463, Training Accuracy= 0.85000\n",
      "Epoch   28 , batch Loss= 0.431439, Training Accuracy= 0.86000\n",
      "Epoch   29 , batch Loss= 0.450966, Training Accuracy= 0.85000\n",
      "Epoch   30 , batch Loss= 0.237339, Training Accuracy= 0.92000\n",
      "Epoch   31 , batch Loss= 0.581541, Training Accuracy= 0.82000\n",
      "Epoch   32 , batch Loss= 0.528997, Training Accuracy= 0.85000\n",
      "Epoch   33 , batch Loss= 0.198446, Training Accuracy= 0.93000\n",
      "Epoch   34 , batch Loss= 0.497124, Training Accuracy= 0.88000\n",
      "Epoch   35 , batch Loss= 0.250834, Training Accuracy= 0.93000\n",
      "Epoch   36 , batch Loss= 0.433023, Training Accuracy= 0.85000\n",
      "Epoch   37 , batch Loss= 0.712025, Training Accuracy= 0.84000\n",
      "Epoch   38 , batch Loss= 0.464753, Training Accuracy= 0.86000\n",
      "Epoch   39 , batch Loss= 0.449764, Training Accuracy= 0.87000\n",
      "Epoch   40 , batch Loss= 0.220164, Training Accuracy= 0.97000\n",
      "Epoch   41 , batch Loss= 0.273117, Training Accuracy= 0.92000\n",
      "Epoch   42 , batch Loss= 0.351515, Training Accuracy= 0.90000\n",
      "Epoch   43 , batch Loss= 0.325763, Training Accuracy= 0.87000\n",
      "Epoch   44 , batch Loss= 0.375544, Training Accuracy= 0.93000\n",
      "Epoch   45 , batch Loss= 0.324976, Training Accuracy= 0.92000\n",
      "Epoch   46 , batch Loss= 0.277689, Training Accuracy= 0.91000\n",
      "Epoch   47 , batch Loss= 0.331883, Training Accuracy= 0.89000\n",
      "Epoch   48 , batch Loss= 0.168608, Training Accuracy= 0.93000\n",
      "Epoch   49 , batch Loss= 0.110252, Training Accuracy= 0.98000\n",
      "Epoch   50 , batch Loss= 0.231946, Training Accuracy= 0.95000\n",
      "Epoch   51 , batch Loss= 0.229287, Training Accuracy= 0.94000\n",
      "Epoch   52 , batch Loss= 0.447223, Training Accuracy= 0.83000\n",
      "Epoch   53 , batch Loss= 0.193253, Training Accuracy= 0.94000\n",
      "Epoch   54 , batch Loss= 0.138801, Training Accuracy= 0.97000\n",
      "Epoch   55 , batch Loss= 0.083551, Training Accuracy= 1.00000\n",
      "Epoch   56 , batch Loss= 0.145167, Training Accuracy= 0.97000\n",
      "Epoch   57 , batch Loss= 0.249043, Training Accuracy= 0.90000\n",
      "Epoch   58 , batch Loss= 0.235423, Training Accuracy= 0.95000\n",
      "Epoch   59 , batch Loss= 0.217349, Training Accuracy= 0.96000\n",
      "Epoch   60 , batch Loss= 0.127335, Training Accuracy= 0.95000\n",
      "Epoch   61 , batch Loss= 0.145579, Training Accuracy= 0.97000\n",
      "Epoch   62 , batch Loss= 0.240445, Training Accuracy= 0.94000\n",
      "Epoch   63 , batch Loss= 0.132699, Training Accuracy= 0.97000\n",
      "Epoch   64 , batch Loss= 0.092527, Training Accuracy= 0.97000\n",
      "Epoch   65 , batch Loss= 0.308895, Training Accuracy= 0.91000\n",
      "Epoch   66 , batch Loss= 0.311775, Training Accuracy= 0.88000\n",
      "Epoch   67 , batch Loss= 0.343991, Training Accuracy= 0.91000\n",
      "Epoch   68 , batch Loss= 0.209954, Training Accuracy= 0.94000\n",
      "Epoch   69 , batch Loss= 0.120212, Training Accuracy= 0.97000\n",
      "Epoch   70 , batch Loss= 0.201517, Training Accuracy= 0.96000\n",
      "Epoch   71 , batch Loss= 0.111950, Training Accuracy= 0.97000\n",
      "Epoch   72 , batch Loss= 0.231343, Training Accuracy= 0.89000\n",
      "Epoch   73 , batch Loss= 0.270026, Training Accuracy= 0.93000\n",
      "Epoch   74 , batch Loss= 0.182627, Training Accuracy= 0.93000\n",
      "Epoch   75 , batch Loss= 0.264102, Training Accuracy= 0.93000\n",
      "Epoch   76 , batch Loss= 0.352203, Training Accuracy= 0.88000\n",
      "Epoch   77 , batch Loss= 0.182559, Training Accuracy= 0.97000\n",
      "Epoch   78 , batch Loss= 0.179050, Training Accuracy= 0.93000\n",
      "Epoch   79 , batch Loss= 0.262165, Training Accuracy= 0.90000\n",
      "Epoch   80 , batch Loss= 0.343106, Training Accuracy= 0.87000\n",
      "Epoch   81 , batch Loss= 0.292591, Training Accuracy= 0.91000\n",
      "Epoch   82 , batch Loss= 0.128112, Training Accuracy= 0.98000\n",
      "Epoch   83 , batch Loss= 0.204745, Training Accuracy= 0.95000\n",
      "Epoch   84 , batch Loss= 0.146712, Training Accuracy= 0.96000\n",
      "Epoch   85 , batch Loss= 0.145640, Training Accuracy= 0.96000\n",
      "Epoch   86 , batch Loss= 0.201261, Training Accuracy= 0.95000\n",
      "Epoch   87 , batch Loss= 0.252927, Training Accuracy= 0.93000\n",
      "Epoch   88 , batch Loss= 0.133692, Training Accuracy= 0.97000\n",
      "Epoch   89 , batch Loss= 0.291014, Training Accuracy= 0.92000\n",
      "Epoch   90 , batch Loss= 0.147041, Training Accuracy= 0.95000\n",
      "Epoch   91 , batch Loss= 0.157401, Training Accuracy= 0.97000\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    for epoch in range(training_epochs):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y\n",
    "                                                          })\n",
    "        print \"Epoch  \" ,epoch  ,\", batch Loss= \" + \\\n",
    "              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy mnist test images\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                      y: mnist.test.labels\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
